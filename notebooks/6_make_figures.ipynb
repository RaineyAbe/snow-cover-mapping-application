{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a268cb2-8765-4785-a416-f29c9289f371",
   "metadata": {},
   "source": [
    "# Make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886bd1-2d50-401f-aaef-8353f32db569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e6df1-03f5-472a-8c1c-222f8f92ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "# scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "eras_fn = os.path.join(scm_path, 'analysis', 'all_era_data.csv')\n",
    "clusters_fn = os.path.join(scm_path, 'analysis', 'climate_clusters.csv')\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7489240-51f6-4b35-8c16-4927ccc6c615",
   "metadata": {},
   "source": [
    "## Define some colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c52145-2f0b-4149-8645-7e855d2118db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate clusters\n",
    "cluster_cmap_dict = {'W. Aleutians': '#dd3497', \n",
    "                     'Continental': '#a6611a',\n",
    "                     'Transitional-Continental': '#dfc27d',\n",
    "                     'Transitional-Maritime': '#80cdc1',\n",
    "                     'Maritime': '#018571'}\n",
    "\n",
    "cmap = plt.cm.Greys\n",
    "n = 10\n",
    "subregions_cmap_dict_grey = {'Alaska Range': cmap(0/n),\n",
    "                             'Aleutians': cmap(1/n),\n",
    "                             'W. Chugach Mtns.': cmap(2/n),\n",
    "                             'St. Elias Mtns.': cmap(3/n),\n",
    "                             'N. Coast Ranges': cmap(4/n),\n",
    "                             'N. Rockies': cmap(5/n),\n",
    "                             'N. Cascades': cmap(6/n),\n",
    "                             'C. Rockies': cmap(7/n),\n",
    "                             'S. Cascades': cmap(8/n),\n",
    "                             'S. Rockies': cmap(9/n)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cf12a-9f84-44c5-a7a3-b198eafb234b",
   "metadata": {},
   "source": [
    "## Define order of clusters and subregions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28883b-0bc0-4d7e-8b92-64d6f5be83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_order = ['W. Aleutians', 'Maritime', 'Transitional-Maritime', 'Transitional-Continental', 'Continental']\n",
    "subregion_order = ['N. Rockies', 'Alaska Range', 'W. Chugach Mtns.', 'St. Elias Mtns.', 'N. Coast Ranges',\n",
    "                   'Aleutians', 'N. Cascades', 'C. Rockies', 'S. Cascades', 'S. Rockies']\n",
    "# separate subregions for plotting\n",
    "group1 = ['N. Rockies', 'W. Chugach Mtns.', 'N. Coast Ranges', 'N. Cascades', 'S. Cascades']\n",
    "group2 = ['Alaska Range', 'St. Elias Mtns.', 'Aleutians', 'C. Rockies', 'S. Rockies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6f855-6b1b-46cc-b4a5-e8e2c27763ae",
   "metadata": {},
   "source": [
    "## Figure 1. Median AARs and timing comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df22e2b-3bf1-4602-ace4-d954143b840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('mako', n_colors=len(subregion_order)+1)\n",
    "cmap_dict = dict([[subregion, color] for subregion, color in list(zip(subregion_order, cmap))])\n",
    "\n",
    "# -----Load glacier boundaries\n",
    "aois = gpd.read_file(aois_fn)\n",
    "print('Glacier boundaries loaded')\n",
    "\n",
    "# -----Load climate clusters\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "# -----Load median AARs for all sites\n",
    "min_scs_fn = os.path.join(scm_path, 'analysis', 'min_snow_cover_stats.csv')\n",
    "min_scs = pd.read_csv(min_scs_fn)\n",
    "# Add difference from September AAR\n",
    "# min_scs['AAR_P50_diff'] = min_scs['AAR_P50_WOY39'] - min_scs['AAR_P50_min']\n",
    "# Add Subregion and climate cluster info\n",
    "min_scs[['CenLon', 'CenLat', 'Subregion', 'clustName']] = 0, 0, '', ''\n",
    "for rgi_id in min_scs['RGIId'].drop_duplicates().values:\n",
    "    cenlon, cenlat, subregion = aois.loc[aois['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion']].values[0]\n",
    "    cluster = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values\n",
    "    min_scs.loc[min_scs['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion']] = cenlon, cenlat, subregion\n",
    "# Sort by subregion order\n",
    "min_scs['Subregion'] = pd.Categorical(min_scs['Subregion'], subregion_order)\n",
    "min_scs.sort_values(by='Subregion', inplace=True)\n",
    "print('Median AARs loaded')\n",
    "\n",
    "# -----Load melt season timings estimate\n",
    "melt_season_fn = os.path.join(scm_path, 'analysis', 'melt_season_timing.csv')\n",
    "melt_season = pd.read_csv(melt_season_fn)\n",
    "# Add subregion column\n",
    "melt_season['Subregion'] = ''\n",
    "for rgi_id in melt_season['RGIId'].drop_duplicates().values:\n",
    "    subregion = aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values[0]\n",
    "    melt_season.loc[melt_season['RGIId']==rgi_id, 'Subregion'] = subregion\n",
    "# Sort by subregion order\n",
    "melt_season['Subregion'] = pd.Categorical(melt_season['Subregion'], subregion_order)\n",
    "melt_season.sort_values(by='Subregion', inplace=True)\n",
    "print('Melt season timing loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f7edb-9d20-4c6f-8687-2c135b4ed769",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "plt.rcParams.update({'font.size': fontsize, 'font.sans-serif':'Arial'})\n",
    "gs = matplotlib.gridspec.GridSpec(3,10, wspace=0)\n",
    "fig = plt.figure(figsize=(8,14))\n",
    "ax = []\n",
    "\n",
    "# Iterate over subregions\n",
    "median_color = 'w'\n",
    "Iax = -1\n",
    "for i, subregion in enumerate(min_scs['Subregion'].unique()):\n",
    "    # a) AARs\n",
    "    ax.append(fig.add_subplot(gs[0,i]))\n",
    "    Iax += 1\n",
    "    min_scs_subregion = min_scs.loc[min_scs['Subregion']==subregion]\n",
    "    sns.boxplot(data=min_scs_subregion, x=i, y='AAR', showfliers=False,\n",
    "                color=cmap_dict[subregion], medianprops=dict(color='w', linewidth=1.5), ax=ax[Iax])\n",
    "    ax[Iax].set_ylim(-0.01,1)\n",
    "    ax[Iax].set_xticks([])\n",
    "    ax[Iax].spines[['right', 'top']].set_visible(False)\n",
    "    if i==0:\n",
    "        ax[Iax].set_ylabel('AAR')\n",
    "        ax[Iax].text(0.2, 0.9, 'a', transform=ax[Iax].transAxes, \n",
    "                     fontweight='bold', fontsize=fontsize+3, color='k')\n",
    "    else:\n",
    "        ax[Iax].set_ylabel('')\n",
    "        ax[Iax].set_yticks([])\n",
    "        ax[Iax].spines['left'].set_visible(False)\n",
    "    ax[Iax].set_title(subregion, rotation=90, color=cmap_dict[subregion], fontweight='bold')\n",
    "    \n",
    "    # b) AAR timing and melt season duration\n",
    "    ax.append(fig.add_subplot(gs[1,i]))\n",
    "    Iax += 1\n",
    "    k = sns.kdeplot(min_scs_subregion['WOY'], vertical=True, color=cmap_dict[subregion], \n",
    "                    fill=True, alpha=1, ax=ax[Iax], zorder=2)\n",
    "    ax[Iax].set_ylim(13,45)\n",
    "    median = min_scs_subregion['WOY'].median()\n",
    "    ax[Iax].plot([0, ax[Iax].get_xlim()[1]*0.9], [median, median], '-', color='w', zorder=3)\n",
    "    ax[Iax].plot([0,0], [15,45], '-', color=cmap_dict[subregion], linewidth=2)\n",
    "    melt_season_subregion = melt_season.loc[melt_season['Subregion']==subregion]\n",
    "    melt_season_start = melt_season_subregion['melt_season_start_WOY'].mean()\n",
    "    melt_season_end = melt_season_subregion['melt_season_end_WOY'].mean()\n",
    "    ax[Iax].fill_between([0, ax[Iax].get_xlim()[1]], \n",
    "                         [melt_season_start, melt_season_start],  \n",
    "                         [melt_season_end, melt_season_end],\n",
    "                         facecolor='k', edgecolor='None', alpha=0.2, zorder=1, label='Melt season duration')\n",
    "    if i==4:\n",
    "        ax[Iax].legend(loc='upper center', frameon=False, bbox_to_anchor=[0.87, 0.92, 0.2, 0.2])\n",
    "    ax[Iax].spines[['left', 'right', 'top', 'bottom']].set_visible(False)\n",
    "    ax[Iax].set_yticks([18, 22, 26, 31, 35, 39, 44])\n",
    "    ax[Iax].set_ylabel('')\n",
    "    ax[Iax].set_xticks([])\n",
    "    ax[Iax].set_xlabel('')\n",
    "    if i==0:\n",
    "        ax[Iax].set_yticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov'])\n",
    "        ax[Iax].set_ylabel('Snow minimum timing')\n",
    "        ax[Iax].text(0.2, 0.9, 'b', transform=ax[Iax].transAxes, \n",
    "                     fontweight='bold', fontsize=fontsize+3, color='k')\n",
    "    else:\n",
    "        ax[Iax].set_yticklabels([])\n",
    "        \n",
    "    # c) Observed vs. Sept 1 AAR\n",
    "    # ax.append(fig.add_subplot(gs[2,i]))\n",
    "    # Iax += 1\n",
    "    # min_scs_subregion_melt = pd.melt(\n",
    "    #     min_scs_subregion,\n",
    "    #     id_vars=['RGIId', 'Subregion'],\n",
    "    #     value_vars=['AAR_P50_min', f'AAR_P50_WOY39'],\n",
    "    #     var_name='AAR_type',\n",
    "    #     value_name='AAR'\n",
    "    # )\n",
    "    # vp = sns.violinplot(data=min_scs_subregion_melt, y='AAR', hue='AAR_type', cut=0, orient='v',\n",
    "    #                     split=True, inner=\"quart\", fill=True, ax=ax[Iax],\n",
    "    #                     linewidth=1, dodge=True, width=0.4,\n",
    "    # )#palette=cmap_dict)\n",
    "    # violins = vp.collections\n",
    "    # ax[Iax].set_ylim(-0.01,1)\n",
    "    # ax[Iax].set_xlabel('')\n",
    "    # ax[Iax].set_xticks([])\n",
    "    # ax[Iax].spines[['right', 'top']].set_visible(False)\n",
    "    # handles, labels = ax[Iax].get_legend_handles_labels()\n",
    "    # ax[Iax].legend().remove()\n",
    "    # if i==0:\n",
    "    #     ax[Iax].text(0.2, 0.9, 'c', transform=ax[Iax].transAxes, \n",
    "    #                  fontweight='bold', fontsize=fontsize+3, color='k')\n",
    "    # else:\n",
    "    #     ax[Iax].set_yticks([])\n",
    "    #     ax[Iax].set_ylabel('')\n",
    "    #     ax[Iax].spines['left'].set_visible(False)\n",
    "    # if i==4:\n",
    "    #     labels = ['Observed AAR', 'September 1 SAR']\n",
    "    #     ax[Iax].legend(handles, labels, loc='upper center', ncols=2, \n",
    "    #                    frameon=False, bbox_to_anchor=[0.8, 0.92, 0.2, 0.2])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "# fig_fn = os.path.join(figures_out_path, 'fig01_median_aars+timings.png')\n",
    "# fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "# print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5d7b5-d596-4908-bf9b-2821e4b1dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check WOY and months\n",
    "# df = pd.DataFrame({'Date': pd.date_range('2013-01-01', '2023-12-30')})\n",
    "# df['WOY'] = df['Date'].dt.isocalendar().week\n",
    "# df['year'] = df['Date'].dt.year\n",
    "# df['month'] = df['Date'].dt.month\n",
    "# df['day'] = df['Date'].dt.day\n",
    "# df.loc[(df['month']==10) & (df['day']==1)]['WOY']#.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bf271-88ea-4056-9048-04652a4b339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "\n",
    "print('All sites:')\n",
    "print('-----------')\n",
    "print('---AARs---')\n",
    "print(min_scs['AAR'].describe())\n",
    "print('\\n---WOYs---')\n",
    "print(min_scs['WOY'].describe()) \n",
    "\n",
    "print(' ')\n",
    "print('By subregion:')\n",
    "print('-----------')\n",
    "print('---AARs---')\n",
    "print(min_scs.groupby('Subregion')['AAR'].describe())\n",
    "\n",
    "print('\\n---WOYs---')\n",
    "print(min_scs.groupby('Subregion')['WOY'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76ef00-65c4-465e-84f1-2bcf3ce753bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at which sites had AARs < 0.1\n",
    "aar_lt_p1 = min_scs.loc[min_scs['AAR'] < 0.1]\n",
    "print(len(aar_lt_p1))\n",
    "aar_lt_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca717b",
   "metadata": {},
   "source": [
    "## Figure 2. Melt factors of snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "print('AOIs loaded')\n",
    "\n",
    "# Load climate clusters / mean climate\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Climate clusters loaded')\n",
    "\n",
    "# Load melt factors\n",
    "fsnow_obs_fn = os.path.join(scm_path, 'analysis', 'fsnow_observed.csv')\n",
    "fsnow_obs = pd.read_csv(fsnow_obs_fn)\n",
    "# add centroid coordinates and climate cluster\n",
    "fsnow_obs[['clustName', 'Subregion', 'CenLon', 'CenLat']] = '', '', 0, 0\n",
    "for rgi_id in fsnow_obs['RGIId'].drop_duplicates().values:\n",
    "    cluster = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    cenLon, cenLat, subregion = aois.loc[aois['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion']].values[0]\n",
    "    fsnow_obs.loc[fsnow_obs['RGIId']==rgi_id, ['clustName', 'Subregion', 'CenLon', 'CenLat']] = cluster, subregion, cenLon, cenLat\n",
    "print('Melt factors of snow loaded')\n",
    "\n",
    "# Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "print('RGI O2 regions loaded')\n",
    "\n",
    "# Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Plot\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(10,14))\n",
    "gs = matplotlib.gridspec.GridSpec(2, 2, height_ratios=[1.5,1])\n",
    "ax = [fig.add_subplot(gs[0,:]),\n",
    "      fig.add_subplot(gs[1,0]),\n",
    "      fig.add_subplot(gs[1,1])]\n",
    "\n",
    "### a) Map view\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 67)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_aspect(2.1)\n",
    "# Melt factors\n",
    "sns.scatterplot(data=fsnow_obs, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.5, \n",
    "                hue='clustName', palette=cluster_cmap_dict, alpha=1, size='fsnow_obs', \n",
    "                sizes=(5,100), ax=ax[0])\n",
    "# handles, labels = ax[0].get_legend_handles_labels()\n",
    "# labels = [x.replace('clustName', 'Climate class') for x in labels]\n",
    "ax[0].legend().remove()\n",
    "# fig.legend(handles, labels, loc='center right', bbox_to_anchor=[0.7, 0.38, 0.2, 0.2], fontsize=fontsize-1)\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax[0].text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "\n",
    "### b) Mean weather conditions\n",
    "sns.scatterplot(data=clusters, x='mean_annual_temp_range', y='mean_annual_precip_cumsum', \n",
    "                hue='clustName', palette=cluster_cmap_dict, alpha=1, ax=ax[1], s=15, legend=False)\n",
    "ax[1].set_xlabel('Air temperature range [$^{\\circ}$C]', fontsize=fontsize)\n",
    "ax[1].set_ylabel('Precipitation sum [m]', fontsize=fontsize)\n",
    "\n",
    "### c) Boxplots\n",
    "sns.boxplot(fsnow_obs, x='clustName', y='fsnow_obs', hue='clustName', \n",
    "            palette=cluster_cmap_dict, showfliers=False, ax=ax[2])\n",
    "ax[2].legend().remove()\n",
    "ax[2].set_xticks([])\n",
    "ax[2].set_xlabel('Climate cluster')\n",
    "ax[2].set_ylabel('f$_{snow}$ []')\n",
    "\n",
    "# Add text labels\n",
    "text_labels = ['a', 'b', 'c']\n",
    "for i, axis in enumerate(ax):\n",
    "    if i==1:\n",
    "        scale = 0.85\n",
    "    else:\n",
    "        scale = 0.9\n",
    "    axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*scale + axis.get_xlim()[0],\n",
    "              (axis.get_ylim()[1] - axis.get_ylim()[0])*scale + axis.get_ylim()[0],\n",
    "              text_labels[i], fontsize=fontsize+4, fontweight='bold')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.15)\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "# fig_fn = os.path.join(figures_out_path, 'fig02_ELA_PDD_sensitivities.png')\n",
    "# fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "# print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a06df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7f5a8d-e625-417b-a49a-9b095c761729",
   "metadata": {},
   "source": [
    "## Figure S4. PDD coefficient correlations with elevation range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc6d6-6082-4878-a486-2b3791a2564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,14))\n",
    "gs = matplotlib.gridspec.GridSpec(6, 2, height_ratios=[2,1,1,1,1,1])\n",
    "ax = [fig.add_subplot(gs[0,:]),\n",
    "      fig.add_subplot(gs[1,0]), fig.add_subplot(gs[1,1]),\n",
    "      fig.add_subplot(gs[2,0]), fig.add_subplot(gs[2,1]),\n",
    "      fig.add_subplot(gs[3,0]), fig.add_subplot(gs[3,1]),\n",
    "      fig.add_subplot(gs[4,0]), fig.add_subplot(gs[4,1]),\n",
    "      fig.add_subplot(gs[5,0]), fig.add_subplot(gs[5,1])]\n",
    "\n",
    "# function for linear regression\n",
    "def linear_regression(X,y):\n",
    "    lr = LinearRegression().fit(X, y)\n",
    "    score = lr.score(X, y)\n",
    "    Xpred = np.linspace(np.min(X), np.max(X), 50).reshape(-1, 1)\n",
    "    ypred = lr.predict(Xpred)\n",
    "    return Xpred, ypred, score\n",
    "\n",
    "# All subregions\n",
    "sns.scatterplot(data=fits_obs_df, x='coef_PDD_median', y='Zrange_km', hue='clustName',\n",
    "                palette=cluster_cmap_dict, ax=ax[0], legend=False, sizes=100)\n",
    "X = fits_obs_df['coef_PDD_median'].values.reshape(-1, 1)\n",
    "y = fits_obs_df['Zrange_km'].values\n",
    "Xpred, ypred, score = linear_regression(X,y)\n",
    "ax[0].plot(Xpred, ypred, '-k', alpha=0.5)\n",
    "ax[0].set_ylabel('')\n",
    "ax[0].set_xlabel('')\n",
    "ax[0].text(ax[0].get_xlim()[0] + (ax[0].get_xlim()[1] - ax[0].get_xlim()[0]) * 0.95,\n",
    "          ax[0].get_ylim()[0] + (ax[0].get_ylim()[1] - ax[0].get_ylim()[0]) * 0.1,\n",
    "         f'R$^2$ = {np.round(score,3)}', ha = 'right')\n",
    "ax[0].set_title('a) All regions')\n",
    "\n",
    "# Individual subregions\n",
    "text_labels = ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']\n",
    "for i, subregion in enumerate(subregion_order):\n",
    "    fits_obs_subregion_df = fits_obs_df.loc[fits_obs_df['Subregion']==subregion]\n",
    "    # coefficient vs. Zrange\n",
    "    sns.scatterplot(data=fits_obs_subregion_df, x='coef_PDD_median', y='Zrange_km', hue='clustName',\n",
    "                    palette=cluster_cmap_dict, ax=ax[i+1], legend=False, sizes=100)\n",
    "    # linear regression\n",
    "    X = fits_obs_subregion_df['coef_PDD_median'].values.reshape(-1, 1)\n",
    "    y = fits_obs_subregion_df['Zrange_km'].values\n",
    "    Xpred, ypred, score = linear_regression(X,y)\n",
    "    ax[i+1].plot(Xpred, ypred, '-k', alpha=0.5)\n",
    "    ax[i+1].set_ylabel('')\n",
    "    ax[i+1].set_xlabel('')\n",
    "    ax[i+1].text(ax[i+1].get_xlim()[0] + (ax[i+1].get_xlim()[1] - ax[i+1].get_xlim()[0]) * 0.95,\n",
    "             ax[i+1].get_ylim()[0] + (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.1,\n",
    "             f'R$^2$ = {np.round(score,3)}', ha = 'right')\n",
    "    ax[i+1].set_title(f'{text_labels[i]}) {subregion}')\n",
    "\n",
    "# Add axis labels\n",
    "for i in [0, 1, 3, 5, 7, 9]:\n",
    "    ax[i].set_ylabel('Elevation range [km]')\n",
    "for i in [9, 10]:\n",
    "    ax[i].set_xlabel('$\\Sigma$PDD coefficient [m/$^{\\circ}$C]')\n",
    "        \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'figS4_coefPDDmedian_vs_Zrange.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ad611-f047-4fae-95c5-865070735b4d",
   "metadata": {},
   "source": [
    "## Figure 3. ELAs: observed/modeled comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9f0ca-49f0-4d0f-b177-9a01e0e34ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load merged monthly transient and annual ELAs \n",
    "slas_merged_monthly_fn = os.path.join(scm_path, 'analysis', 'monthly_SLAs_modeled_observed_merged.csv')\n",
    "slas_merged_monthly = pd.read_csv(slas_merged_monthly_fn)\n",
    "slas_merged_monthly['Date'] = pd.to_datetime(slas_merged_monthly['Date'])\n",
    "slas_merged_monthly['Month'] = pd.DatetimeIndex(slas_merged_monthly['Date']).month\n",
    "elas_merged_annual_fn = os.path.join(scm_path, 'analysis', 'annual_ELAs_modeled_observed_merged.csv')\n",
    "elas_merged_annual = pd.read_csv(elas_merged_annual_fn)\n",
    "print('Merged ELAs loaded')\n",
    "\n",
    "# AOIs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('Compiled AOIs loaded')\n",
    "\n",
    "# Load climate clusters\n",
    "clusters_fn = os.path.join(scm_path, 'analysis', 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Clusters loaded')\n",
    "\n",
    "# Add subregion and cluster names to ELA dfs\n",
    "slas_merged_monthly[['Subregion', 'clustName']] = '', ''\n",
    "elas_merged_annual[['Subregion', 'clustName']] = '', ''\n",
    "for rgi_id in aois['RGIId'].drop_duplicates().values:\n",
    "    cluster = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    subregion = aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values[0]\n",
    "    slas_merged_monthly.loc[slas_merged_monthly['RGIId']==rgi_id, ['Subregion', 'clustName']] = subregion, cluster\n",
    "    elas_merged_annual.loc[elas_merged_annual['RGIId']==rgi_id, ['Subregion', 'clustName']] = subregion, cluster \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b035d-5552-4cf2-aa56-a8cefb9b37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5), gridspec_kw=dict(width_ratios=[2,1]))\n",
    "\n",
    "# Monthly snowline differences\n",
    "slas_merged_monthly['SLA_mod-obs_m'] = slas_merged_monthly['SLA_mod_m'] - slas_merged_monthly['SLA_obs_m']\n",
    "sns.boxplot(data=slas_merged_monthly, x='Month', y='SLA_mod-obs_m', hue='clustName', hue_order=cluster_order,\n",
    "            palette=cluster_cmap_dict, saturation=1, showfliers=False,\n",
    "            boxprops=dict(linewidth=1, edgecolor='k'), whiskerprops=dict(linewidth=1, color='k'), \n",
    "            ax=ax[0])\n",
    "ax[0].set_xticks(np.arange(0,5))\n",
    "ax[0].set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "ax[0].set_ylim(-1500, 1500)\n",
    "ax[0].set_title('a) Modeled $-$ Observed snowline altitudes')\n",
    "ax[0].set_ylabel('Snowline altitude difference [m]')\n",
    "\n",
    "# ELA differences\n",
    "elas_merged_annual['ELA_mod-obs_m'] = elas_merged_annual['ELA_mod_m'] - elas_merged_annual['ELA_obs_m']\n",
    "sns.kdeplot(data=elas_merged_annual, y='ELA_mod-obs_m', hue='clustName', hue_order=cluster_order, \n",
    "            palette=cluster_cmap_dict, ax=ax[1])\n",
    "# Add median lines\n",
    "# xmin, xmax = ax[1].get_xlim()\n",
    "# for clustName in elas_merged_annual['clustName'].drop_duplicates().values:\n",
    "#     med = elas_merged_annual.loc[elas_merged_annual['clustName']==clustName, 'ELA_mod-obs_m'].median()\n",
    "#     ax[1].axhline(y=med, xmin=0, xmax=1, color=cluster_cmap_dict[clustName], linestyle='-', linewidth=1)\n",
    "ax[1].set_ylim(-1500, 1500)\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].set_title('b) Modeled $-$ Observed ELAs')\n",
    "ax[1].set_ylabel('ELA difference [m]')\n",
    "ax[1].set_xticks([])\n",
    "\n",
    "# Add legend\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend().remove()\n",
    "ax[1].legend().remove()\n",
    "fig.legend(handles, labels, ncols=len(cluster_order), loc='lower center',\n",
    "           bbox_to_anchor=[0.43, -0.1, 0.2, 0.2], columnspacing=0.8, handletextpad=0.25)\n",
    "\n",
    "# Add text label and line at 0\n",
    "text_labels = ['a', 'b']\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.axhline(0, color='k')\n",
    "    # axis.text((axis.get_xlim()[1]-axis.get_xlim()[0])*0.9 + axis.get_xlim()[0],\n",
    "    #           (axis.get_ylim()[1]-axis.get_ylim()[0])*0.9 + axis.get_ylim()[0],\n",
    "    #           text_labels[i], fontsize=fontsize+4, fontweight='bold')\n",
    "    \n",
    "# fig.subplots_adjust(wspace=0.2)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'fig03_ELAs_observed_modeled_differences.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ce886-bc3f-48c0-8716-4e5f7f0fd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot boxplots by subregion and cluster\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "subtitles = ['Monthly transient ELAs', 'Annual ELAs']\n",
    "for subfig, subtitle, df in zip(subfigs, subtitles, [slas_merged_monthly, elas_merged_annual]):\n",
    "    subfig.suptitle(subtitle)\n",
    "    axs = subfig.subplots(nrows=1, ncols=2)\n",
    "    for axis, group in zip(axs, [group1, group2]):\n",
    "        df_group = df.loc[df['Subregion'].isin(group)]\n",
    "        sns.boxplot(df_group, x='ELA_obs-mod_m', y='Subregion', showfliers=False,\n",
    "                    hue='clustName', palette=cluster_cmap_dict, order=group, hue_order=cluster_order,\n",
    "                    boxprops=dict(linewidth=1), ax=axis)\n",
    "        axis.legend().remove()\n",
    "        axis.yaxis.set_minor_locator(matplotlib.ticker.FixedLocator(np.arange(-0.5, 9.5, step=1)))\n",
    "        axis.yaxis.grid(True, which='minor')\n",
    "        axis.set_ylabel('')\n",
    "        axis.set_xlabel('Remotely-sensed $-$ Modeled [m]')\n",
    "        axis.set_xlim(-2000, 2000)\n",
    "        axis.axvline(0, color='grey')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858b44f-472d-492e-87bc-ff3d04df7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLA stats\n",
    "print(slas_merged_monthly.groupby(['Month'])['ELA_obs-mod_m'].describe())\n",
    "slas_merged_monthly.groupby(['clustName', 'Month'])['ELA_obs-mod_m'].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELA stats\n",
    "print(elas_merged_annual['ELA_obs-mod_m'].describe())\n",
    "elas_merged_annual.groupby('clustName')['ELA_obs-mod_m'].describe().sort_values(by='50%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df2069-b7e2-4295-9b60-e566d1e88a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot monthly histograms\n",
    "slas_merged_monthly['Month'] = pd.DatetimeIndex(slas_merged_monthly['Date']).month\n",
    "slas_merged_monthly['Difference'] = slas_merged_monthly['ELA_obs_m'] - slas_merged_monthly['ELA_mod_m']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(data=slas_merged_monthly, x='Month', y='Difference', hue='clustName', \n",
    "            palette=cluster_cmap_dict, showfliers=False, ax=ax, saturation=1)\n",
    "sns.move_legend(ax, loc='center right', bbox_to_anchor=[1.35, 0.5, 0.2, 0.2], title='Climate class')\n",
    "ax.set_ylabel('Snowline elevation difference [m]\\n(Remotely-sensed$-$Modeled)')\n",
    "ax.set_xticks(np.arange(0,6))\n",
    "ax.set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct'])\n",
    "ax.axhline(0, color='k')\n",
    "plt.show()\n",
    "\n",
    "## Plot weekly histograms\n",
    "slas_merged_monthly['Week'] = slas_merged_monthly['Date'].dt.isocalendar().week\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(data=slas_merged_monthly, x='Week', y='Difference', hue='clustName', \n",
    "            palette=cluster_cmap_dict, showfliers=False, ax=ax, saturation=1)\n",
    "sns.move_legend(ax, loc='center right', bbox_to_anchor=[1.35, 0.5, 0.2, 0.2], title='Climate class')\n",
    "ax.set_ylabel('Snowline elevation difference [m]\\n(Remotely-sensed$-$Modeled)')\n",
    "# ax.set_xticks(np.arange(0,6))\n",
    "# ax.set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct'])\n",
    "ax.axhline(0, color='k')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d16aaf-de80-4b97-af97-29651c5f68b3",
   "metadata": {},
   "source": [
    "## Figure S1. Sites distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba3fc8-3053-4e30-a488-99df69cce4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analyzed glacier boundaires\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "cols = ['O1Region', 'O2Region', 'Zmed', 'Aspect', 'Slope', 'Area']\n",
    "for col in cols:\n",
    "    aois[col] = aois[col].astype(float)\n",
    "aois['Subregion'] = ''\n",
    "for o1region, o2region in aois[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    aois.loc[(aois['O1Region']==o1region) & (aois['O2Region']==o2region), 'Subregion'] = subregion_name\n",
    "\n",
    "    \n",
    "# Load all glacier boundaries in O1 regions 1 and 2\n",
    "rgi_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/RGI/'\n",
    "rgi_fns = ['01_rgi60_Alaska/01_rgi60_Alaska.shp',\n",
    "           '02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp']\n",
    "rgi = gpd.GeoDataFrame()\n",
    "for rgi_fn in rgi_fns:\n",
    "    file = gpd.read_file(os.path.join(rgi_path, rgi_fn))\n",
    "    rgi = pd.concat([rgi, file])\n",
    "rgi[['O1Region', 'O2Region']] = rgi[['O1Region', 'O2Region']].astype(int)\n",
    "# Add column for subregion name\n",
    "rgi['Subregion'] = ''\n",
    "for o1region, o2region in rgi[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    rgi.loc[(rgi['O1Region']==o1region) & (rgi['O2Region']==o2region), 'Subregion'] = subregion_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d356a8-0d7c-48cd-bc3a-fea96869aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting variables\n",
    "columns = ['Zmed', 'Aspect', 'Slope', 'Area']\n",
    "xlabels = ['Median elevation [m]', 'Aspect [degrees]', 'Slope [degrees]', 'Area [km$^2$]']\n",
    "bins_list = [np.linspace(0, 361, num=20), # Aspect\n",
    "             np.linspace(0, 51, num=20), # Slope\n",
    "             np.linspace(0, 300, num=50)] # Area\n",
    "\n",
    "# Set up figure\n",
    "plt.rcParams.update({'font.sans-serif': 'Arial', 'font.size': 12})\n",
    "fig, ax = plt.subplots(len(subregion_order)+1, 4, figsize=(12, (len(subregion_order)+1)*1.5))\n",
    "aois_color = '#b35806'\n",
    "\n",
    "# All subregions\n",
    "for j, (column, xlabel) in enumerate(zip(columns, xlabels)):\n",
    "    if column=='Zmed': \n",
    "        bins = np.linspace(rgi['Zmed'].min(), rgi['Zmed'].max(), num=20)\n",
    "    else:\n",
    "        bins  = bins_list[j-1]\n",
    "    ax[0,j].hist(rgi[column].values, bins=bins, facecolor='k', alpha=0.6)\n",
    "    ax[0,j].set_title(xlabel)\n",
    "    ax2 = ax[0,j].twinx()\n",
    "    ax2.hist(aois[column].values, bins=bins, facecolor=aois_color, alpha=0.6)\n",
    "    ax2.set_yticks(ax2.get_yticks())\n",
    "    ax2.set_yticklabels(ax2.get_yticklabels(), color=aois_color)\n",
    "    ax2.spines['right'].set_color(aois_color)\n",
    "    ax2.tick_params(axis='y', color=aois_color)\n",
    "ax[0,0].set_ylabel('All regions', fontweight='bold')\n",
    "\n",
    "# Individual subregions\n",
    "for i, subregion in enumerate(subregion_order):\n",
    "    # Subset glaciers\n",
    "    aois_subregion = aois.loc[aois['Subregion']==subregion]\n",
    "    rgi_subregion = rgi.loc[rgi['Subregion']==subregion]\n",
    "\n",
    "    # Plot all glaciers in subregion\n",
    "    for j, (column, xlabel) in enumerate(zip(columns, xlabels)):\n",
    "        if column=='Zmed':\n",
    "            bins = np.linspace(rgi_subregion['Zmed'].min(), rgi_subregion['Zmed'].max(), num=20)\n",
    "        else:\n",
    "            bins = bins_list[j-1]\n",
    "        ax[i+1,j].hist(rgi_subregion[column].values, bins=bins, facecolor='k', alpha=0.6)\n",
    "        if j==0:\n",
    "            ax[i+1,j].set_ylabel(subregion)\n",
    "        ax2 = ax[i+1,j].twinx()\n",
    "        ax2.hist(aois_subregion[column].values, bins=bins, facecolor=aois_color, alpha=0.6)\n",
    "        ax2.set_yticks(ax2.get_yticks())\n",
    "        ax2.set_yticklabels(ax2.get_yticklabels(), color=aois_color)\n",
    "        ax2.spines['right'].set_color(aois_color)\n",
    "        ax2.tick_params(axis='y', color=aois_color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS1_site_distributions.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967c216-ecb9-4788-b673-9c6a4dfeb447",
   "metadata": {},
   "source": [
    "## Figure S3. Regression scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ddaa1-5369-46ac-8bf2-663bf6b789a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load linear fits\n",
    "fits_obs_fn = os.path.join(scm_path, 'analysis', 'linear_fit_observed_monthly_ela_pdd_snowfall.csv')\n",
    "fits_obs_df = pd.read_csv(fits_obs_fn)\n",
    "# Add center longitude and latitude coordinates from AOIs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "fits_obs_df = pd.merge(fits_obs_df, aois[['RGIId', 'CenLon', 'CenLat']], on='RGIId')\n",
    "# Add weather clusters\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "fits_obs_df = pd.merge(fits_obs_df, clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "print('Linear fits loaded')\n",
    "\n",
    "# -----Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for o1o2 in rgi_O2['o2region'].drop_duplicates().values:\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'color'] = color\n",
    "print('RGI O2 regions loaded from file')\n",
    "\n",
    "# -----Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded from file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178ea13-42f4-4d55-b1c8-0461f7a3edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=16\n",
    "plt.rcParams.update({'font.size': fontsize, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,6), gridspec_kw={'width_ratios': [1,1.4]})\n",
    "\n",
    "# Histogram of regression scores\n",
    "sns.histplot(data=fits_obs_df, x='score_mean', hue='clustName', hue_order=cluster_order, \n",
    "             palette=cluster_cmap_dict, multiple='stack', bins=20, legend=False, ax=ax[0])\n",
    "ax[0].set_xlabel('Mean regression score')\n",
    "\n",
    "# Hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[1].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[1], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "# Map of regression scores\n",
    "sns.scatterplot(data=fits_obs_df, x='CenLon', y='CenLat', hue='clustName', hue_order=cluster_order, \n",
    "                palette=cluster_cmap_dict, size='score_mean', sizes=(1,75), edgecolor='#525252', \n",
    "                linewidth=1, legend='brief', ax=ax[1])\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "# weather class legend\n",
    "fig.legend(handles[1:6], labels[1:6], loc='upper center', ncols=5, markerscale=2, \n",
    "           bbox_to_anchor=[0.4, 0.88, 0.2, 0.2], frameon=False, handletextpad=0.4)\n",
    "# marker sizes legend\n",
    "handles, labels = handles[7:], labels[7:]\n",
    "leg = ax[1].legend(handles, labels, title=\"Mean regression score\", \n",
    "                   loc='lower left', alignment='left', bbox_to_anchor=[0.2, 0.01, 0.2, 0.2])\n",
    "ax[1].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[1].set_xlim(-167, -112)\n",
    "ax[1].set_ylim(46, 67)\n",
    "ax[1].set_xlabel('Longitude')\n",
    "ax[1].set_ylabel('Latitude')\n",
    "ax[1].set_aspect(2)\n",
    "\n",
    "# Add text labels\n",
    "ax[0].text(ax[0].get_xlim()[0] + (ax[0].get_xlim()[1] - ax[0].get_xlim()[0])*0.9,\n",
    "           ax[0].get_ylim()[0] + (ax[0].get_ylim()[1] - ax[0].get_ylim()[0])*0.9,\n",
    "           'a', fontweight='bold', fontsize=fontsize+4)\n",
    "ax[1].text(ax[1].get_xlim()[0] + (ax[1].get_xlim()[1] - ax[1].get_xlim()[0])*0.93,\n",
    "           ax[1].get_ylim()[0] + (ax[1].get_ylim()[1] - ax[1].get_ylim()[0])*0.9,\n",
    "           'b', fontweight='bold', fontsize=fontsize+4)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'figS3_regression_scores.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n",
    "\n",
    "fits_obs_df[['score_mean']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b1073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8cfb9d-1f3f-4796-98ee-ec16a2cabf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_obs_df.loc[fits_obs_df['score_mean'] < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197c75d3",
   "metadata": {},
   "source": [
    "## Table S1. Study sites with subregion and weather cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "aois = gpd.read_file(aois_fn)\n",
    "aois.rename(columns={'Subregion': 'Subregion name'}, inplace=True)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "\n",
    "# Add weather cluster\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "aois['Weather cluster'] = ''\n",
    "for rgi_id in aois['RGIId'].drop_duplicates().values:\n",
    "    aois.loc[aois['RGIId']==rgi_id, 'Weather cluster'] = clusters.loc[clusters['RGIId']==rgi_id, 'clustName']\n",
    "aois.sort_values(by=['O1Region', 'O2Region'], inplace=True)\n",
    "\n",
    "# Format as LaTeX table\n",
    "columns = ['RGIId', 'O1Region', 'O2Region', 'Subregion name', 'Weather cluster']\n",
    "aois = aois[columns]\n",
    "\n",
    "# Save as Excel sheet\n",
    "aois_xl_fn = aois_fn.replace('all_aois.shp', 'TableS1_study_sites.xlsx')\n",
    "aois.to_excel(aois_xl_fn, index=False)\n",
    "print('Table saved as Excel spreadsheet:', aois_xl_fn)\n",
    "aois\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db3d9b-f9b5-4c28-8ad2-156f59b64e56",
   "metadata": {},
   "source": [
    "## AGU24 abstract figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc3a0c-69fc-44f1-91c2-29989208a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('mako', n_colors=len(subregion_order)+2)\n",
    "\n",
    "# -----Load median AARs for all sites\n",
    "min_snow_cover_stats_fn = os.path.join(scm_path, 'results', 'min_snow_cover_stats.csv')\n",
    "min_snow_cover_stats = pd.read_csv(min_snow_cover_stats_fn)\n",
    "# Sort subregions\n",
    "min_snow_cover_stats['order'] = ''\n",
    "for i, subregion in enumerate(subregion_order):\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['Subregion']==subregion, 'order'] = i\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['Subregion']==subregion, 'color'] = matplotlib.colors.to_hex(cmap[i])\n",
    "min_snow_cover_stats = min_snow_cover_stats.sort_values(by='order')\n",
    "print('Median AARs loaded from file')\n",
    "\n",
    "# # -----Load RGI O2 Regions\n",
    "# rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "#                                 'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "# rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# # remove Brooks Range\n",
    "# rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# # add subregion name and color column\n",
    "# rgi_O2[['Subregion', 'color']] = '', ''\n",
    "# for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "#     o1 = int(o1o2[0:2])\n",
    "#     o2 = int(o1o2[3:])\n",
    "#     subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "#     rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "#     rgi_O2.loc[rgi_O2['o2region']==o1o2, 'color'] = dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values)[subregion]\n",
    "# print('RGI O2 regions loaded from file')\n",
    "\n",
    "# # -----Load GTOPO30\n",
    "# gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "# gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "# gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "# print('GTOPO30 loaded from file')\n",
    "\n",
    "# # -----Load classified image\n",
    "# site_name = 'RGI60-01.00037'\n",
    "# im_classified_fn = f'/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/{site_name}/imagery/classified/20230802T152742_RGI60-01.00037_Sentinel-2_SR_classified.nc'\n",
    "# im_classified = xr.open_dataset(im_classified_fn)\n",
    "# print('Classified image loaded')\n",
    "\n",
    "# # -----Load classified images colormap\n",
    "# import json\n",
    "# datasets_dict_fn = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/inputs-outputs/datasets_characteristics.json'\n",
    "# datasets_dict = json.load(open(datasets_dict_fn))\n",
    "# cmap_classified = matplotlib.colors.ListedColormap(datasets_dict['classified_image']['class_colors'].values())\n",
    "\n",
    "# # -----Load Sentinel-2 image from GEE\n",
    "# import math\n",
    "# import wxee as wx\n",
    "# import geedim as gd\n",
    "# import ee\n",
    "# ee.Initialize()\n",
    "# def convert_wgs_to_utm(lon: float, lat: float):\n",
    "#     utm_band = str((math.floor((lon + 180) / 6) % 60) + 1)\n",
    "#     if len(utm_band) == 1:\n",
    "#         utm_band = '0' + utm_band\n",
    "#     if lat >= 0:\n",
    "#         epsg_code = '326' + utm_band\n",
    "#         return epsg_code\n",
    "#     epsg_code = '327' + utm_band\n",
    "#     return epsg_code\n",
    "# # Load AOI\n",
    "# aoi_fn = glob.glob(f'/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/{site_name}/AOIs/*.shp')[0]\n",
    "# aoi = gpd.read_file(aoi_fn)\n",
    "# aoi_bounds = aoi.geometry[0].bounds\n",
    "# region = ee.Geometry.Polygon([[aoi_bounds[0], aoi_bounds[1]], \n",
    "#                               [aoi_bounds[2], aoi_bounds[1]],\n",
    "#                               [aoi_bounds[2], aoi_bounds[3]],\n",
    "#                               [aoi_bounds[0], aoi_bounds[3]],\n",
    "#                               [aoi_bounds[0], aoi_bounds[1]]])\n",
    "# # Load image collection\n",
    "# im_col = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date='2023-08-01',\n",
    "#                                                                              end_date='2023-08-03',\n",
    "#                                                                              region=region,\n",
    "#                                                                              mask=True)\n",
    "# im_ee = im_col.ee_collection.first()\n",
    "# im_ee = im_ee.clip(region)\n",
    "# im_ee = im_ee.select(['B4', 'B3', 'B2'])\n",
    "# # Convert to xarray.Dataset\n",
    "# im_xr = im_ee.wx.to_xarray(scale=30, crs='EPSG:4326')\n",
    "# im_xr = xr.where(im_xr==im_xr.attrs['_FillValue'], np.nan, im_xr / 1e4)\n",
    "# im_xr = im_xr.rio.write_crs('EPSG:4326')\n",
    "# print('Sentinel-2 SR image loaded')\n",
    "\n",
    "# # Reproject AOI and images to optimal UTM zone\n",
    "# epsg_utm = convert_wgs_to_utm(aoi.geometry[0].centroid.coords.xy[0][0], aoi.geometry[0].centroid.coords.xy[1][0])\n",
    "# aoi_utm = aoi.to_crs(f'EPSG:{epsg_utm}')\n",
    "# im_xr = im_xr.rio.reproject(f'EPSG:{epsg_utm}')\n",
    "# im_classified = im_classified.rio.write_crs(\"EPSG:4326\")\n",
    "# im_classified = im_classified.rio.reproject(f'EPSG:{epsg_utm}')\n",
    "# im_classified = xr.where(im_classified < 1, np.nan, im_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa0d1b-715d-45e8-bdb3-593af6912041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=14\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": fontsize,\n",
    "    \"font.sans-serif\": \"Arial\",\n",
    "    # \"font.family\": \"sans-serif\",\n",
    "    # \"font.sans-serif\": \"Computer Modern Sans Serif\",\n",
    "    \"text.usetex\": False\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "gs = matplotlib.gridspec.GridSpec(2, 2, figure=fig, height_ratios=[1,2])\n",
    "ax = [fig.add_subplot(gs[2:]),\n",
    "      fig.add_subplot(gs[0]),\n",
    "      fig.add_subplot(gs[1])]\n",
    "\n",
    "# ----- Study sites\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 67)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_aspect(2.2)\n",
    "# Median AARs\n",
    "sns.scatterplot(data=min_snow_cover_stats, x='CenLon', y='CenLat', edgecolor='w', linewidth=0.5, \n",
    "                hue='Subregion', hue_order=subregion_order, palette=dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values), \n",
    "                alpha=1, size='AAR_P50_min', sizes=(2,100), ax=ax[0])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "Ikeep = np.argwhere(['0.' in x for x in np.array(labels)]).flatten()\n",
    "handles, labels = [handles[i] for i in Ikeep], [labels[i] for i in Ikeep] \n",
    "ax[0].legend(handles, labels, loc='lower left', title='20132023 median AAR', bbox_to_anchor=[0.2, 0.05, 0.2, 0.2])\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax[0].text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-2, fontweight=fontweight)\n",
    "ax[0].text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-2, fontweight=fontweight)\n",
    "# Example site location\n",
    "min_snow_cover_stats_site = min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==site_name]\n",
    "ax[0].plot(min_snow_cover_stats_site['CenLon'], min_snow_cover_stats_site['CenLat'], '*', \n",
    "           markeredgecolor='k', markerfacecolor='#e7298a', markersize=15, linewidth=2)\n",
    "    \n",
    "# -----b) Sentinel-2 image\n",
    "ax[1].imshow(np.dstack([im_xr.B4.data[0], im_xr.B3.data[0], im_xr.B2.data[0]]),\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, \n",
    "                     np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].set_ylabel('Northing [km]')\n",
    "\n",
    "# -----c) Classified image\n",
    "ax[2].imshow(im_classified.classified.data[0], cmap=cmap_classified, clim=(1,5),\n",
    "             extent=(np.min(im_classified.x.data)/1e3, np.max(im_classified.x.data)/1e3,\n",
    "                     np.min(im_classified.y.data)/1e3, np.max(im_classified.y.data)/1e3))\n",
    "ax[2].plot(np.divide(aoi_utm.geometry[0].exterior.coords.xy[0], 1e3), np.divide(aoi_utm.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "           '-k', label='Glacier boundary')\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "# dummy points for legend\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(0), label='Snow')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(1), label='Shadowed snow')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(2), label='Ice/firn')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(3), label='Rock/debris')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(4), label='Water')\n",
    "ax[2].set_xlim(ax[1].get_xlim())\n",
    "ax[2].set_ylim(ax[1].get_ylim())\n",
    "handles, labels = ax[2].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncols=6, markerscale=2, frameon=False)\n",
    "\n",
    "# Plot AOI\n",
    "for axis in ax[1:]:\n",
    "    axis.plot(np.divide(aoi_utm.geometry[0].exterior.coords.xy[0], 1e3), \n",
    "              np.divide(aoi_utm.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "              '-k', linewidth=1, label='Glacier boundary')\n",
    "    axis.set_yticks(np.arange(7030, 7046, step=5))\n",
    "\n",
    "# Add text labels\n",
    "text_labels = ['c', 'a', 'b']\n",
    "for i in range(0, len(ax)):\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0]) * 0.9 + ax[i].get_xlim()[0],\n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0]) * 0.85 + ax[i].get_ylim()[0],\n",
    "                text_labels[i], fontweight='bold', fontsize=fontsize+4, horizontalalignment='center',\n",
    "              bbox=dict(facecolor='w', edgecolor='None', pad=3))\n",
    "\n",
    "# # Add caption\n",
    "# caption = (r\"\\noindent\\textbf{Figure 1. a)} Sentinel-2 surface reflectance image captured 2023-08-02 for one glacier (Randolph Glacier Inventory ID = 1.00037) \\\\\"\n",
    "#             r\"and the associated \\textbf{b)} classified image generated from the automated snow detection pipeline. \\textbf{c)} Map of the study glacier locations, \\\\\"\n",
    "#             r\"with marker sizes indicating the median accumulation area ratio (AAR) for the 20132023 study period. The maroon start marks the \\\\\"\n",
    "#             r\"location of the example glacier shown in panels \\textbf{a} and \\textbf{b}.\" )\n",
    "# fig.text(0.05, -0.02, caption, ha='left', wrap=True, fontsize=fontsize+1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'agu24_abstract_figure.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092b8ae",
   "metadata": {},
   "source": [
    "## Snow cover GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb687748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Load inputs\n",
    "rgi_id = \"RGI60-01.00312\"\n",
    "aoi_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'AOIs', f\"{rgi_id}_outline.shp\")\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "im_classified_fns = sorted(glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'imagery', 'classified', '*.nc')))\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "# subset to 2019\n",
    "im_classified_fns = [x for x in im_classified_fns if int(os.path.basename(x)[0:4]) == 2019] \n",
    "scs = scs.loc[scs['datetime'].dt.year == 2019]\n",
    "out_path = os.path.join(figures_out_path, 'timeseries_gif')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print('Made directory for outputs:', out_path)\n",
    "\n",
    "# Define colormap for classified images\n",
    "cmap_dict = {\"Snow\": \"#4eb3d3\",  \"Shadowed_snow\": \"#636363\", \"Ice\": \"#084081\", \"Rock\": \"#fe9929\", \"Water\": \"#252525\"}\n",
    "colors = []\n",
    "for key in list(cmap_dict.keys()):\n",
    "    color = list(matplotlib.colors.to_rgb(cmap_dict[key]))\n",
    "    if key=='Rock':\n",
    "        color += [0.5]\n",
    "    colors.append(color)\n",
    "        \n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605f4f7-37a4-4238-b334-cb2787c4e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download images for 2019 ###\n",
    "\n",
    "import math\n",
    "import ee\n",
    "import geedim as gd\n",
    "import datetime\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "def convert_wgs_to_utm(lon: float, lat: float):\n",
    "    utm_band = str((math.floor((lon + 180) / 6) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0' + utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = 'EPSG:326' + utm_band\n",
    "        return epsg_code\n",
    "    epsg_code = 'EPSG:327' + utm_band\n",
    "    return epsg_code\n",
    "    \n",
    "def query_gee_for_imagery_run_pipeline(dataset, aoi_utm, date_start, date_end,\n",
    "                                       month_start, month_end, site_name, \n",
    "                                       mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=None,\n",
    "                                       verbose=False, im_download=False):\n",
    "\n",
    "    # -----Grab optimal UTM zone from AOI CRS\n",
    "    epsg_utm = str(aoi_utm.crs.to_epsg())\n",
    "\n",
    "    # -----Reformat AOI for image filtering\n",
    "    # reproject CRS from AOI to WGS\n",
    "    aoi_wgs = aoi_utm.to_crs('EPSG:4326')\n",
    "    # prepare AOI for querying geedim (AOI bounding box)\n",
    "    region = {'type': 'Polygon',\n",
    "              'coordinates': [[[aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]]\n",
    "                               ]]}\n",
    "\n",
    "    # -----Define function to query GEE for imagery\n",
    "    def query_gee(dataset, date_start, date_end, region, cloud_cover_max, mask_clouds):\n",
    "        if dataset == 'Landsat8':\n",
    "            # Landsat 8\n",
    "            im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                       end_date=date_end,\n",
    "                                                                                       region=region,\n",
    "                                                                                       cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                       mask=mask_clouds)\n",
    "        elif dataset == 'Landsat9':\n",
    "            # Landsat 9\n",
    "            im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC09/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                       end_date=date_end,\n",
    "                                                                                       region=region,\n",
    "                                                                                       cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                       mask=mask_clouds)\n",
    "        elif dataset == 'Sentinel-2_TOA':\n",
    "            im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                         end_date=date_end,\n",
    "                                                                                         region=region,\n",
    "                                                                                         cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                         mask=mask_clouds)\n",
    "\n",
    "        elif dataset == 'Sentinel-2_SR':\n",
    "            im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                            end_date=date_end,\n",
    "                                                                                            region=region,\n",
    "                                                                                            cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                            mask=mask_clouds)\n",
    "        else:\n",
    "            print(\"'dataset' variable not recognized. Please set to 'Landsat', 'Sentinel-2_TOA', or 'Sentinel-2_SR'. \"\n",
    "                  \"Exiting...\")\n",
    "            return 'N/A'\n",
    "\n",
    "        return im_col_gd\n",
    "\n",
    "    # -----Define function to filter image IDs by month range\n",
    "    def filter_im_ids_month_range(im_ids, im_dts, month_start, month_end):\n",
    "        i = [int(ii) for ii in np.arange(0, len(im_dts)) if\n",
    "             (im_dts[ii].month >= month_start) and (im_dts[ii].month <= month_end)]  # indices of images to keep\n",
    "        im_ids, im_dts = [im_ids[ii] for ii in i], [im_dts[ii] for ii in i]  # subset of image IDs and datetimes\n",
    "        # return 'N/A' if no images remain after filtering by month range\n",
    "        if len(im_dts) < 1:\n",
    "            return 'N/A', 'N/A'\n",
    "        return im_ids, im_dts\n",
    "\n",
    "    # -----Define function to couple image IDs captured within the same hour for mosaicking\n",
    "    def image_mosaic_ids(im_col_gd):\n",
    "        # Grab image properties, IDs, and datetimes from image collection\n",
    "        properties = im_col_gd.properties\n",
    "        ims = dict(properties).keys()\n",
    "        im_ids = [properties[im]['system:id'] for im in ims]\n",
    "        # return if no images found\n",
    "        if len(im_ids) < 1:\n",
    "            return 'N/A', 'N/A'\n",
    "        im_dts = np.array(\n",
    "            [datetime.datetime.utcfromtimestamp(properties[im]['system:time_start'] / 1000) for im in ims])\n",
    "\n",
    "        # Remove image datetimes and IDs outside the specified month range\n",
    "        im_ids, im_dts = filter_im_ids_month_range(im_ids, im_dts, month_start, month_end)\n",
    "\n",
    "        # Grab all unique hours in image datetimes\n",
    "        hours = np.array(im_dts, dtype='datetime64[h]')\n",
    "        unique_hours = sorted(set(hours))\n",
    "\n",
    "        # Create list of IDs for each unique hour\n",
    "        im_mosaic_ids_list, im_mosaic_dts_list = [], []\n",
    "        for unique_hour in unique_hours:\n",
    "            i = list(np.ravel(np.argwhere(hours == unique_hour)))\n",
    "            im_ids_list_hour = [im_ids[ii] for ii in i]\n",
    "            im_mosaic_ids_list.append(im_ids_list_hour)\n",
    "            im_dts_list_hour = [im_dts[ii] for ii in i]\n",
    "            im_mosaic_dts_list.append(im_dts_list_hour)\n",
    "\n",
    "        return im_mosaic_ids_list, im_mosaic_dts_list\n",
    "\n",
    "    # -----Define function for extracting valid image IDs\n",
    "    def extract_valid_image_ids(ds, date_start, date_end, region, cloud_cover_max, mask_clouds):\n",
    "        # Initialize list of date ranges for querying\n",
    "        date_ranges = [(date_start, date_end)]\n",
    "        # Initialize list of error dates\n",
    "        error_dates = []\n",
    "        # Initialize error flag\n",
    "        error_occurred = True\n",
    "        # Iterate until no errors occur\n",
    "        while error_occurred:\n",
    "            error_occurred = False  # Reset the error flag at the beginning of each iteration\n",
    "            try:\n",
    "                # Initialize list of image collections\n",
    "                im_col_gd_list = []\n",
    "                # Iterate over date ranges\n",
    "                for date_range in date_ranges:\n",
    "                    # Query GEE for imagery\n",
    "                    im_col_gd = query_gee(ds, date_range[0], date_range[1], region, cloud_cover_max, mask_clouds)\n",
    "                    properties = im_col_gd.properties  # Error will occur here if an image is inaccessible!\n",
    "                    im_col_gd_list.append(im_col_gd)\n",
    "                # Initialize list of filtered image IDs and datetimes\n",
    "                im_mosaic_ids_list_full, im_mosaic_dts_list_full = [], []  # Initialize lists of\n",
    "                # Filter image IDs for month range and couple IDs for mosaicking\n",
    "                for im_col_gd in im_col_gd_list:\n",
    "                    im_mosaic_ids_list, im_mosaic_dts_list = image_mosaic_ids(im_col_gd)\n",
    "                    if type(im_mosaic_ids_list) is str:\n",
    "                        return 'N/A', 'N/A'\n",
    "                    # append to list\n",
    "                    im_mosaic_ids_list_full = im_mosaic_ids_list_full + im_mosaic_ids_list\n",
    "                    im_mosaic_dts_list_full = im_mosaic_dts_list_full + im_mosaic_dts_list\n",
    "\n",
    "                return im_mosaic_ids_list_full, im_mosaic_dts_list_full\n",
    "\n",
    "            except Exception as e:\n",
    "                error_id = str(e).split('ID=')[1].split(')')[0]\n",
    "                print(f\"Error querying GEE for {str(error_id)}\")\n",
    "\n",
    "                # Parse the error date from the exception message (replace this with your actual parsing logic)\n",
    "                error_date = datetime.datetime.strptime(error_id[0:8], '%Y%m%d')\n",
    "                error_dates.append(error_date)\n",
    "\n",
    "                # Update date ranges excluding the problematic date\n",
    "                date_starts = [date_start] + [str(error_date + datetime.timedelta(days=1))[0:10] for error_date in\n",
    "                                              error_dates]\n",
    "                date_ends = [str(error_date - datetime.timedelta(days=1))[0:10] for error_date in error_dates] + [\n",
    "                    date_end]\n",
    "                date_ranges = list(zip(date_starts, date_ends))\n",
    "\n",
    "                # Set the error flag to indicate that an error occurred\n",
    "                error_occurred = True\n",
    "\n",
    "    # -----Apply functions\n",
    "    if dataset == 'Landsat':  # must run Landsat 8 and 9 separately\n",
    "        im_ids_list_8, im_dts_list_8 = extract_valid_image_ids('Landsat8', date_start, date_end, region,\n",
    "                                                               cloud_cover_max, mask_clouds)\n",
    "        im_ids_list_9, im_dts_list_9 = extract_valid_image_ids('Landsat9', date_start, date_end, region,\n",
    "                                                               cloud_cover_max, mask_clouds)\n",
    "        if (type(im_ids_list_8) is str) and (type(im_ids_list_9) is str):\n",
    "            im_ids_list, im_dts_list = 'N/A', 'N/A'\n",
    "        elif type(im_ids_list_9) is str:\n",
    "            im_ids_list, im_dts_list = im_ids_list_8, im_dts_list_8\n",
    "        elif type(im_ids_list_8) is str:\n",
    "            im_ids_list, im_dts_list = im_ids_list_9, im_dts_list_9\n",
    "        else:\n",
    "            im_ids_list = im_ids_list_8 + im_ids_list_9\n",
    "            im_dts_list = im_dts_list_8 + im_dts_list_9\n",
    "    else:\n",
    "        im_ids_list, im_dts_list = extract_valid_image_ids(dataset, date_start, date_end, region, cloud_cover_max,\n",
    "                                                           mask_clouds)\n",
    "\n",
    "    # -----Check if any images were found after filtering\n",
    "    if type(im_ids_list) is str:\n",
    "        print('No images found or error in one or more image IDs, exiting...')\n",
    "        return 'N/A'\n",
    "\n",
    "    if dataset=='Landsat':\n",
    "        res = 30\n",
    "        image_scalar = 36363.63636363636\n",
    "        no_data_value = 0\n",
    "    elif 'Sentinel-2' in dataset:\n",
    "        res = 10\n",
    "        image_scalar = 1e4\n",
    "        no_data_value = -9999\n",
    "\n",
    "    # -----Create xarray.Datasets from list of image IDs\n",
    "    # loop through image IDs\n",
    "    for i in tqdm(range(0, len(im_ids_list))):\n",
    "\n",
    "        # subset image IDs and image datetimes\n",
    "        im_ids, im_dts = im_ids_list[i], im_dts_list[i]\n",
    "\n",
    "        # make directory for outputs (out_path) if it doesn't exist\n",
    "        if not os.path.exists(im_out_path):\n",
    "            os.mkdir(im_out_path)\n",
    "            print('Made directory for image downloads: ' + im_out_path)\n",
    "            \n",
    "        # define filename\n",
    "        if len(im_dts) > 1:\n",
    "            im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '_MOSAIC.tif'\n",
    "        else:\n",
    "            im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '.tif'\n",
    "            \n",
    "        # check file does not already exist in directory, download\n",
    "        if not os.path.exists(os.path.join(im_out_path, im_fn)):\n",
    "            # create list of MaskedImages from IDs\n",
    "            im_gd_list = [gd.MaskedImage.from_id(im_id) for im_id in im_ids]\n",
    "            # combine into new MaskedCollection\n",
    "            im_collection = gd.MaskedCollection.from_list(im_gd_list)\n",
    "            # create image composite\n",
    "            im_composite = im_collection.composite(method=gd.CompositeMethod.q_mosaic,\n",
    "                                                   mask=mask_clouds,\n",
    "                                                   region=region)\n",
    "            # download to file\n",
    "            im_composite.download(os.path.join(im_out_path, im_fn),\n",
    "                                  region=region,\n",
    "                                  scale=res,\n",
    "                                  crs='EPSG:4326',\n",
    "                                  dtype='int16',\n",
    "                                  bands=im_composite.refl_bands)\n",
    "\n",
    "    return\n",
    "\n",
    "# Reproject AOI to optimal UTM zone\n",
    "epsg_utm = convert_wgs_to_utm(aoi.geometry[0].centroid.coords.xy[0][0], aoi.geometry[0].centroid.coords.xy[1][0])\n",
    "aoi_utm = aoi.to_crs(epsg_utm)\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "# Landsat\n",
    "dataset = 'Landsat'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Sentinel-2 SR\n",
    "dataset = 'Sentinel-2_SR'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Sentinel-2 SR\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Remove images with < 70% coverage of AOI\n",
    "fns = sorted(glob.glob(os.path.join(out_path, '*.tif')))\n",
    "fn_remove_list = []\n",
    "for fn in tqdm(fns):\n",
    "    im = rxr.open_rasterio(fn).squeeze()\n",
    "    im = im.rio.reproject(epsg_utm)\n",
    "    im = im.isel(band=0)\n",
    "    mask = geometry_mask(aoi_utm.geometry, transform=im.rio.transform(), invert=True, out_shape=im.shape)\n",
    "    im_masked = xr.where(mask==1, im, np.nan)\n",
    "    im_masked = xr.where((im_masked==-9999) | (im_masked==-32768), np.nan, im_masked)\n",
    "    nreal = im_masked.notnull().sum().item()\n",
    "    npx = np.count_nonzero(mask)\n",
    "    percentage_real = (nreal / npx) * 100\n",
    "    if percentage_real < 70:\n",
    "        fn_remove_list.append(fn)\n",
    "# for fn in fn_remove_list:\n",
    "#     os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over classified images\n",
    "for im_classified_fn in tqdm(im_classified_fns):\n",
    "    # Load classified image\n",
    "    im_classified = rxr.open_rasterio(im_classified_fn, decode_times=False).squeeze()\n",
    "    im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "    im_classified = xr.where(im_classified==2, 1, im_classified)\n",
    "    date = pd.Timestamp(datetime.datetime.strptime(os.path.basename(im_classified_fn).split('_')[0], \"%Y%m%dT%H%M%S\"))\n",
    "    source = os.path.basename(im_classified_fn).split(rgi_id + '_')[1].split('_classified.nc')[0]\n",
    "\n",
    "    # Load multispec image\n",
    "    try:\n",
    "        im_fn = glob.glob(os.path.join(out_path, f\"{source}_{str(date)[0:10].replace('-', '')}*.tif\"))[0]\n",
    "    except:\n",
    "        continue\n",
    "    im = rxr.open_rasterio(im_fn).squeeze()\n",
    "    if source=='Landsat':\n",
    "        image_scaler = 36363.63636363636\n",
    "    else:\n",
    "        image_scaler = 1e4\n",
    "    im = xr.where((im==-9999) | (im==-32768), np.nan, im / image_scaler)\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 2, figure=fig, height_ratios=[2,1])\n",
    "    ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1]), fig.add_subplot(gs[1,:])]\n",
    "    # RGB image\n",
    "    ax[0].imshow(np.dstack([im.isel(band=2).data, im.isel(band=1).data, im.isel(band=0).data]),\n",
    "                 extent=(np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)))\n",
    "    aoi.plot(ax=ax[0], facecolor='None', edgecolor='k')\n",
    "    # classified image\n",
    "    xmin, xmax = -145.3555, -145.045\n",
    "    ymin, ymax = 63.144, 63.325\n",
    "    ax[1].imshow(im_classified.data, cmap=cmap, clim=(1,5), \n",
    "                 extent=(xmin, xmax, ymax, ymin))\n",
    "    ax[1].invert_yaxis()\n",
    "    aoi.plot(ax=ax[1], facecolor='None', edgecolor='k')\n",
    "    # dummy points for legend\n",
    "    ax[1].plot(0, 0, 's', color=colors[0], markersize=12, label='Snow')\n",
    "    ax[1].plot(0, 0, 's', color=colors[2], markersize=12, label='Ice')\n",
    "    ax[1].plot(0, 0, 's', color=colors[3], markersize=12, label='Rock')\n",
    "    ax[1].plot(0, 0, 's', color=colors[4], markersize=12, label='Water')\n",
    "    ax[1].legend(loc='lower right', frameon=False)\n",
    "    for axis in ax[0:2]:\n",
    "        axis.set_xlim(xmin, xmax)\n",
    "        axis.set_ylim(ymin, ymax)\n",
    "        axis.set_xticks(axis.get_xticks()[1::2])\n",
    "        axis.set_yticks(axis.get_yticks()[1::2])\n",
    "\n",
    "    # AAR time series\n",
    "    ax[2].plot(scs['datetime'], scs['AAR'], '.k')\n",
    "    scs_date = scs.loc[(scs['datetime']==date) & (scs['source']==source)]\n",
    "    ax[2].plot(scs_date['datetime'], scs_date['AAR'], '*m', markersize=15)\n",
    "    ax[2].set_ylim(0,1.05)\n",
    "    ax[2].grid(True)\n",
    "    ax[2].set_ylabel('Snow area ratio')\n",
    "    fig.suptitle(f\"{date}\\n{source.replace('_', ' ')}\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig_fn = os.path.join(out_path, f\"{date}_{rgi_id}_snow_cover.png\")\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f05c0",
   "metadata": {},
   "source": [
    "## Model SLA animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif': 'Arial'})\n",
    "\n",
    "# Define output directory\n",
    "out_path = os.path.join(figures_out_path, 'model_SMB_to_SLA_animation')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print('Made directory for outputs:', out_path)\n",
    "    \n",
    "# Grab modeled SMB file names\n",
    "rgi_id = '1.00032'\n",
    "bin_fn = sorted(glob.glob(os.path.join(scm_path, 'Rounce_et_al_2023', 'binned', f'{rgi_id}*.nc')))[0]\n",
    "\n",
    "# Load binned data\n",
    "bin = xr.open_dataset(bin_fn)\n",
    "# grab data variables\n",
    "h = bin.bin_surface_h_initial.data[0] # surface elevation [m]\n",
    "x = bin.bin_distance.data[0]\n",
    "b_sum = np.zeros((len(bin.time.data), len(h))) # cumulative SMB\n",
    "times = [np.datetime64(x) for x in bin.time.data] # datetimes\n",
    "months = list(pd.DatetimeIndex(times).month) # months of each datetime\n",
    "# iterate over each time period after 2013\n",
    "times = [time for time in times if time >= np.datetime64('2012-10-01')]\n",
    "elas = np.nan*np.zeros(len(times)) # initialize transient ELAs\n",
    "for j, time in enumerate(tqdm(times)):\n",
    "    # subset binned data to time\n",
    "    bin_time = bin.isel(time=j)\n",
    "    # grab the SMB \n",
    "    b_sum[j,:] = bin_time.bin_massbalclim_monthly.data[0]\n",
    "    # add the previous SMB (restart the count in October)\n",
    "    if months[j] != 10: \n",
    "        b_sum[j,:] += b_sum[j-1,:]\n",
    "    # If all SMB > 0, ELA = minimum elevation\n",
    "    if all(b_sum[j,:] > 0):\n",
    "        elas[j] = np.min(h)\n",
    "    # If SMB is > 0 and < 0 in some places, linearly interpolate ELA\n",
    "    elif any(b_sum[j,:] < 0) & any(b_sum[j,:] > 0):\n",
    "        elas[j] = np.interp(0, np.flip(b_sum[j,:]), np.flip(h))\n",
    "    # If SMB < 0 everywhere, fit a piecewise linear fit and extrapolate for SMB=0\n",
    "    elif all(b_sum[j,:] < 0):\n",
    "        X, y = b_sum[j,:], h\n",
    "        elas[j] = np.nanmax(h)\n",
    "    else:\n",
    "        print('issue')\n",
    "        \n",
    "    # Plot results\n",
    "    if time >= np.datetime64('2013-01-01'):\n",
    "        fig, ax = plt.subplots(2, 1, gridspec_kw=dict(height_ratios=[3,1]), figsize=(6,6))\n",
    "        # surface profile\n",
    "        ax[0].fill_between(x, np.zeros(len(x)), h, color='gray', edgecolor='k', alpha=0.5)\n",
    "        positive_mask = b_sum[j,:] > 0\n",
    "        negative_mask = b_sum[j,:] < 0\n",
    "        # positive mass balance bars\n",
    "        ax[0].bar(x[positive_mask], b_sum[j,positive_mask]*50, width=126, \n",
    "            bottom=h[positive_mask], color='blue', alpha=0.6, label='Positive Mass Balance')\n",
    "        # negative mass balance bars\n",
    "        ax[0].bar(x[negative_mask], b_sum[j,negative_mask]*50, width=126, \n",
    "                bottom=h[negative_mask], color='red', alpha=0.6, label='Negative Mass Balance')\n",
    "        # SLA\n",
    "        ax[0].axhline(elas[j], 0, np.nanmax(x), color='k')\n",
    "        ax[0].text(7e3, elas[j]+50, 'Snowline altitude', color='k', ha='right')\n",
    "        ax[0].set_ylim(1.25e3, 3.3e3)\n",
    "        ax[0].set_yticks([1500, 2000, 2500, 3000])\n",
    "        ax[0].set_ylabel('Elevation [m]')\n",
    "        ax[0].set_xlim(0, np.nanmax(x))\n",
    "        ax[0].set_xticks(np.arange(0, 7.1e3, step=1e3))\n",
    "        ax[0].set_xticklabels(np.divide(ax[0].get_xticks(), 1e3).astype(int).astype(str))\n",
    "        ax[0].set_xlabel('km')\n",
    "        ax[0].spines[['top', 'right']].set_visible(False)\n",
    "        ax[0].set_title(str(time)[0:7])\n",
    "        # SLA time series\n",
    "        ax[1].plot(times, elas, '.k', markersize=5)\n",
    "        ax[1].plot(times[j], elas[j], '*m', markersize=10)\n",
    "        ax[1].set_xlim(np.datetime64('2013-01-01'), np.datetime64('2023-01-01'))\n",
    "        ax[1].set_ylim(1330, 2600)\n",
    "        ax[1].set_ylabel('Snowline altitude [m]')\n",
    "        fig.tight_layout()\n",
    "        # Save figure\n",
    "        fig_fn = os.path.join(out_path, f\"{str(time)[0:7]}.png\")\n",
    "        fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b49eb-8ad2-4b6e-9a5f-c9cedc5c7960",
   "metadata": {},
   "source": [
    "## ELA sensitivity to air temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da0c19-8499-462c-9f0d-64aefea5c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "print('Glacier boundaries loaded')\n",
    "\n",
    "# -----Load hypsometric indexes\n",
    "his_fn = os.path.join(scm_path, 'analysis', 'hypsometric_indexes.csv')\n",
    "his = pd.read_csv(his_fn)\n",
    "print('Hypsometric indexes loaded')\n",
    "\n",
    "# -----Load climate clusters\n",
    "mean_climate_fn = os.path.join(scm_path, 'analysis', 'climate_clusters.csv')\n",
    "mean_climate = pd.read_csv(mean_climate_fn)\n",
    "if 'site_name' in list(mean_climate.columns):\n",
    "    mean_climate.rename(columns={'site_name': 'RGIId'}, inplace=True)\n",
    "print('Mean weather conditions loaded')\n",
    "\n",
    "# -----Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "print('RGI O2 regions loaded')\n",
    "\n",
    "# -----Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded')\n",
    "\n",
    "# -----Load remotely-sensed ELA coefficients\n",
    "fits_obs_fn = os.path.join(scm_path, 'analysis', 'linear_fit_observed_monthly_ela_pdd_snowfall.csv')\n",
    "fits_obs_df = pd.read_csv(fits_obs_fn)\n",
    "fits_obs_df[['CenLon', 'CenLat', 'Area', 'Zrange', 'Zmin', 'clustName']] = 0, 0, 0, 0, 0, ''\n",
    "for rgi_id in fits_obs_df['RGIId'].drop_duplicates().values:\n",
    "    cen_lon, cen_lat, area, zmin, zmax = aois.loc[aois['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Area', 'Zmin', 'Zmax']].values[0]\n",
    "    clustName = mean_climate.loc[mean_climate['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    zrange = zmax-zmin\n",
    "    fits_obs_df.loc[fits_obs_df['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Area', 'Zrange', 'Zmin', 'clustName']] = cen_lon, cen_lat, area, zrange, zmin, clustName\n",
    "    \n",
    "# Sort by cluster order for plotting\n",
    "fits_obs_df['clustName'] = pd.Categorical(fits_obs_df['clustName'], cluster_order)\n",
    "fits_obs_df.sort_values(by='clustName', inplace=True)\n",
    "fits_obs_df.reset_index(drop=True, inplace=True)\n",
    "print('Remotely-sensed ELA coefficients loaded') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8863e56-1666-4a9f-b71a-9591f2256ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_obs_df['Zrange_km'] = fits_obs_df['Zrange'] / 1e3\n",
    "\n",
    "# -----Plot\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(10,14))\n",
    "gs1 = matplotlib.gridspec.GridSpec(2, 2, height_ratios=[1.7,1], width_ratios=[8,1])\n",
    "gs2 = matplotlib.gridspec.GridSpec(2, 2, height_ratios=[1.7,1])\n",
    "ax = [fig.add_axes([0.0, 0.3, 0.6, 0.6]), \n",
    "      fig.add_axes([0.66, 0.623, 0.27, 0.15]),\n",
    "      fig.add_subplot(gs2[1,:])]#, fig.add_subplot(gs2[1,1])]\n",
    "\n",
    "### a) Map view\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 67)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_aspect(2.1)\n",
    "# PDD coefficients\n",
    "sns.scatterplot(data=fits_obs_df, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.5, \n",
    "                hue='clustName', palette=cluster_cmap_dict, alpha=1, size='Zrange_km', \n",
    "                sizes=(5,100), ax=ax[0])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "labels = [x.replace('clustName', 'Weather class').replace('coef_PDD_median', '$\\Sigma$PDD coefficient [m/$^{\\circ}$C]').replace('Zrange_km', 'Elevation range [km]')\n",
    "          for x in labels]\n",
    "ax[0].legend().remove()\n",
    "fig.legend(handles, labels, loc='center right', bbox_to_anchor=[0.7, 0.38, 0.2, 0.2], fontsize=fontsize-1)\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax[0].text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "\n",
    "### b) Mean weather conditions\n",
    "sns.scatterplot(data=mean_climate, x='mean_annual_temp_range', y='mean_annual_precip_cumsum', \n",
    "                hue='clustName', palette=cluster_cmap_dict, alpha=1, ax=ax[1], s=15, legend=False)\n",
    "ax[1].set_xlabel('Air temperature range [$^{\\circ}$C]', fontsize=fontsize)\n",
    "ax[1].set_ylabel('Precipitation sum [m]', fontsize=fontsize)\n",
    "\n",
    "### c) Scatterplots\n",
    "sns.scatterplot(fits_obs_df, x='coef_PDD_median', y='Subregion', hue='clustName', size='Zrange_km', edgecolor='k', linewidth=0.5,\n",
    "                sizes=(5,100), palette=cluster_cmap_dict, hue_order=cluster_order, ax=ax[2])\n",
    "ax[2].legend().remove()\n",
    "ax[2].yaxis.set_minor_locator(matplotlib.ticker.FixedLocator(np.arange(-0.5, 9.5, step=1)))\n",
    "ax[2].tick_params(which='minor', length=0)\n",
    "ax[2].yaxis.grid(True, which='minor')\n",
    "ax[2].set_ylabel('')\n",
    "ax[2].set_xlabel('$\\Sigma$PDD coefficient [m/$^{\\circ}$C]')\n",
    "ax[2].set_xlim(0, 7)\n",
    "\n",
    "# Add text labels\n",
    "text_labels = ['a', 'b', 'c']\n",
    "for i, axis in enumerate(ax):\n",
    "    if i==1:\n",
    "        scale = 0.85\n",
    "    else:\n",
    "        scale = 0.9\n",
    "    axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*scale + axis.get_xlim()[0],\n",
    "              (axis.get_ylim()[1] - axis.get_ylim()[0])*scale + axis.get_ylim()[0],\n",
    "              text_labels[i], fontsize=fontsize+4, fontweight='bold')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.15)\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig02_ELA_PDD_sensitivities.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n",
    "\n",
    "# Print statistics by subregion and climate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_obs_df.groupby(['Subregion', 'clustName'])['coef_PDD_median'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f484a7-fdce-47b2-b2d6-982563dc511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanpercentile(fits_obs_df.loc[(fits_obs_df['Subregion']=='S. Rockies') | (fits_obs_df['Subregion']=='N. Rockies') | (fits_obs_df['Subregion']=='C. Rockies'), 'coef_PDD_median'], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769eeed-7877-4dce-a1bb-ff34f048a6a1",
   "metadata": {},
   "source": [
    "### Filter by regression score to see if trends persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794de534-f399-4c19-9637-752389a0650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_obs_filt_df = fits_obs_df.loc[fits_obs_df['score_mean'] > 0.2]\n",
    "\n",
    "# -----Plot\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(10,14))\n",
    "gs1 = matplotlib.gridspec.GridSpec(2, 2, height_ratios=[1.7,1], width_ratios=[8,1])\n",
    "gs2 = matplotlib.gridspec.GridSpec(2, 1, height_ratios=[1.7,1])\n",
    "ax = [fig.add_axes([0.0, 0.3, 0.6, 0.6]), \n",
    "      fig.add_axes([0.66, 0.623, 0.27, 0.15]),\n",
    "      fig.add_subplot(gs2[1,0])]\n",
    "\n",
    "### a) Map view\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 67)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_aspect(2.1)\n",
    "# PDD coefficients\n",
    "sns.scatterplot(data=fits_obs_filt_df, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.7, \n",
    "                hue='clustName', palette=cluster_cmap_dict, alpha=1, size='Zrange_km', \n",
    "                sizes=(5,100), ax=ax[0])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "labels = [x.replace('clustName', 'Weather class').replace('Zrange_km', 'Elevation range [km]') \n",
    "          for x in labels]\n",
    "ax[0].legend().remove()\n",
    "# Size legend in axis\n",
    "# ax[0].legend(handles[6:], labels[6:], loc='lower left', fontsize=fontsize-2, bbox_to_anchor=[0.15, 0.05, 0.2, 0.2])\n",
    "# Weather class legend outside axis\n",
    "fig.legend(handles, labels, loc='center right', bbox_to_anchor=[0.7, 0.38, 0.2, 0.2], fontsize=fontsize-1)\n",
    "\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax[0].text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "\n",
    "### b) Mean weather conditions\n",
    "sns.scatterplot(data=mean_climate, x='mean_annual_temp_range', y='mean_annual_precip_cumsum', \n",
    "                hue='clustName', palette=cluster_cmap_dict, alpha=1, ax=ax[1], s=15, legend=False)\n",
    "ax[1].set_xlabel('Air temperature range [$^{\\circ}$C]', fontsize=fontsize)\n",
    "ax[1].set_ylabel('Precipitation sum [m]', fontsize=fontsize)\n",
    "\n",
    "### c) and d) Box plots by subregion and climate class\n",
    "sns.scatterplot(data=fits_obs_filt_df, x='coef_PDD_median', y='Subregion', hue='clustName', size='Zrange_km',\n",
    "                alpha=0.9, palette=cluster_cmap_dict, sizes=(5,100), legend=False)\n",
    "ax[2].set_ylabel('')\n",
    "ax[2].set_xlabel('$\\Sigma$PDD coefficient [m/$^{\\circ}$C]')\n",
    "ax[2].grid(axis='x')\n",
    "ax[2].set_xlim(0,7)\n",
    "# add minor gridlines on y-axis\n",
    "ax[2].yaxis.set_minor_locator(matplotlib.ticker.FixedLocator(np.arange(-0.5, 9.5, step=1)))\n",
    "ax[2].tick_params(which='minor', length=0)\n",
    "ax[2].yaxis.grid(True, which='minor')\n",
    "\n",
    "# Add text labels\n",
    "text_labels = ['a', 'b', 'c']\n",
    "for i, axis in enumerate(ax):\n",
    "    if i==1:\n",
    "        scale = 0.85\n",
    "    else:\n",
    "        scale = 0.9\n",
    "    axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*scale + axis.get_xlim()[0],\n",
    "              (axis.get_ylim()[1] - axis.get_ylim()[0])*scale + axis.get_ylim()[0],\n",
    "              text_labels[i], fontsize=fontsize+4, fontweight='bold')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.15)\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig02_ELA_PDD_sensitivities_R2_gt_0.2.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032aa306-bb7e-4c94-ae71-74be17225a1b",
   "metadata": {},
   "source": [
    "## AAR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba091c-ac8e-4ec1-ab11-9c43bcf65934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Add Year and WOY columns to snowlines\n",
    "snowlines['Year'] = snowlines['datetime'].dt.isocalendar().year\n",
    "snowlines['WOY'] = snowlines['datetime'].dt.isocalendar().week\n",
    "\n",
    "# -----Add subregion and climate cluster columns to snowlines\n",
    "snowlines[['Subregion', 'color']] = '', '', \n",
    "for o1, o2 in tqdm(aois[['O1Region', 'O2Region']].drop_duplicates().values):\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1) & (aois['O2Region']==o2)]\n",
    "    site_names = aois_subregion['RGIId'].drop_duplicates().values\n",
    "    for site_name in site_names:\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'O1Region'] = o1\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'O2Region'] = o2\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'Subregion'] = subregion_name\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'color'] = color\n",
    "snowlines[['cluster', 'clustName']] = '', '', \n",
    "for site_name in (climate_clusters['site_name'].drop_duplicates().values):\n",
    "    climate_clusters_site = climate_clusters.loc[climate_clusters['site_name']==site_name]\n",
    "    snowlines.loc[snowlines['site_name']==site_name, 'cluster'] = climate_clusters_site['cluster'].values[0]\n",
    "    snowlines.loc[snowlines['site_name']==site_name, 'clustName'] = climate_clusters_site['clustName'].values[0]\n",
    "snowlines = snowlines.loc[snowlines['cluster']!='']\n",
    "snowlines.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bbf70-58ae-4d5c-b0c3-57dc8fe5b3fa",
   "metadata": {},
   "source": [
    "### By subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bcfef-fecc-4a12-8f64-5f5b971f871b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fig, ax = plt.subplots(10, 2, figsize=(12, 40), gridspec_kw={'width_ratios':[4,1]})\n",
    "ax = ax.flatten()\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "q1, q3 = 0.25, 0.75\n",
    "\n",
    "# -----Iterate over subregions\n",
    "o1o2s = snowlines[['O1Region', 'O2Region']].drop_duplicates().dropna().sort_values(by=['O1Region', 'O2Region']).values\n",
    "ymin, ymax = -0.1, 1.1\n",
    "for i, (o1, o2) in enumerate(o1o2s):\n",
    "    # subset snowlines to subregion\n",
    "    snowlines_subregion = snowlines.loc[(snowlines['O1Region']==o1) & (snowlines['O2Region']==o2)]\n",
    "    if len(snowlines_subregion) < 1:\n",
    "        continue\n",
    "    subregion_name, color = snowlines_subregion[['Subregion', 'color']].values[0]\n",
    "    if color=='':\n",
    "        color='k'\n",
    "    # calculate moving median over time\n",
    "    ts_gb = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR']\n",
    "    ts = ts_gb.median().reset_index()\n",
    "    ts = add_date_column(ts)\n",
    "    ts.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    ts['AAR_Q1'] = ts_gb.quantile(q1).reset_index()['AAR']\n",
    "    ts['AAR_Q3'] = ts_gb.quantile(q3).reset_index()['AAR']\n",
    "    # calculate moving median for all years stacked\n",
    "    weekly_gb = snowlines_subregion.groupby(by='WOY')['AAR']\n",
    "    weekly = weekly_gb.median().reset_index()\n",
    "    weekly.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    weekly['AAR_Q1'] = weekly_gb.quantile(q1).reset_index()['AAR']\n",
    "    weekly['AAR_Q3'] = weekly_gb.quantile(q3).reset_index()['AAR']\n",
    "    weekly['WOY'] = weekly['WOY'].values.astype(float)\n",
    "    # plot\n",
    "    # add grey boxes where observations don't exist\n",
    "    for year in ts['Year'].values:\n",
    "        t1, t2 = pd.to_datetime(str(year-1)+'-11-01'), pd.to_datetime(str(year)+'-05-01')\n",
    "        ax[i*2].add_patch(Rectangle((t1, ymin), width=t2-t1, height=ymax-ymin, facecolor='#d9d9d9', edgecolor='None'))\n",
    "        ts_year = ts.loc[ts['Year']==year]\n",
    "        ax[i*2].plot(ts_year['date'], ts_year['AAR_median'], 'o', color=color, linewidth=0.5, \n",
    "                     markerfacecolor=color, markeredgecolor='w', markersize=5)\n",
    "    ax[i*2].set_ylim(ymin, ymax)\n",
    "    ax[i*2].grid()\n",
    "    ax[i*2].set_ylabel('AAR')\n",
    "    N = len(snowlines_subregion['site_name'].drop_duplicates())\n",
    "    ax[i*2].set_title(f'{subregion_name} (N={N})')\n",
    "    ax[i*2].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    ax[(i*2)+1].set_ylim(ymin, ymax)\n",
    "    ax[(i*2)+1].grid()\n",
    "    ax[(i*2)+1].fill_between(weekly['WOY'], weekly['AAR_Q1'], weekly['AAR_Q3'], color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly['AAR_median'], '-', linewidth=2, color=color)\n",
    "    # change week numbers to month labels\n",
    "    ax[(i*2)+1].set_xticks([22, 31, 40])\n",
    "    ax[(i*2)+1].set_xticklabels(['June', 'Aug', 'Oct'])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_subregions.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a405b-9c21-49ed-9ddb-db00161db02c",
   "metadata": {},
   "source": [
    "### By climate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57b313-26ef-485a-a2e8-f61b6a6164b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "n_clusters = len(snowlines['cluster'].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(n_clusters, 2, figsize=(12, 4*n_clusters), gridspec_kw={'width_ratios':[4,1]})\n",
    "ax = ax.flatten()\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "q1, q3 = 0.25, 0.75\n",
    "\n",
    "# -----Iterate over subregions\n",
    "ymin, ymax = -0.1, 1.1\n",
    "for i, cluster_name in enumerate(['Oceanic', \n",
    "                                  'Continental', \n",
    "                                  'Transitional-Continental', \n",
    "                                  'Transitional-Temperate', \n",
    "                                  'Temperate']):\n",
    "    # subset snowlines to cluster\n",
    "    snowlines_cluster = snowlines.loc[snowlines['clustName']==cluster_name]\n",
    "    if len(snowlines_cluster) < 1:\n",
    "        continue\n",
    "    # grab cluster name\n",
    "    color = cluster_cmap_dict[cluster_name]\n",
    "    # calculate moving median over time\n",
    "    ts_gb = snowlines_cluster.groupby(by=['Year', 'WOY'])['AAR']\n",
    "    ts = ts_gb.median().reset_index()\n",
    "    ts = add_date_column(ts)\n",
    "    ts.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    ts['AAR_Q1'] = ts_gb.quantile(q1).reset_index()['AAR']\n",
    "    ts['AAR_Q3'] = ts_gb.quantile(q3).reset_index()['AAR']\n",
    "    # calculate moving median for all years stacked\n",
    "    weekly_gb = snowlines_cluster.groupby(by='WOY')['AAR']\n",
    "    weekly = weekly_gb.median().reset_index()\n",
    "    weekly.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    weekly['AAR_Q1'] = weekly_gb.quantile(q1).reset_index()['AAR']\n",
    "    weekly['AAR_Q3'] = weekly_gb.quantile(q3).reset_index()['AAR']\n",
    "    weekly['WOY'] = weekly['WOY'].values.astype(float)\n",
    "    # plot\n",
    "    # add grey boxes where observations don't exist\n",
    "    for year in ts['Year'].values:\n",
    "        t1, t2 = pd.to_datetime(str(year-1)+'-11-01'), pd.to_datetime(str(year)+'-05-01')\n",
    "        ax[i*2].add_patch(Rectangle((t1, ymin), width=t2-t1, height=ymax-ymin, facecolor='#d9d9d9', edgecolor='None'))\n",
    "        ts_year = ts.loc[ts['Year']==year]\n",
    "        ax[i*2].plot(ts_year['date'], ts_year['AAR_median'], 'o', color=color, linewidth=0.5, \n",
    "                     markerfacecolor=color, markeredgecolor='w', markersize=5)\n",
    "    ax[i*2].set_ylim(ymin, ymax)\n",
    "    ax[i*2].grid()\n",
    "    ax[i*2].set_ylabel('AAR')\n",
    "    N = len(snowlines_cluster['site_name'].drop_duplicates())\n",
    "    ax[i*2].set_title(f'{cluster_name} (N={N})')\n",
    "    ax[i*2].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    ax[(i*2)+1].set_ylim(ymin, ymax)\n",
    "    ax[(i*2)+1].grid()\n",
    "    ax[(i*2)+1].fill_between(weekly['WOY'], weekly['AAR_Q1'], weekly['AAR_Q3'], color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly['AAR_median'], '-', linewidth=2, color=color)\n",
    "    # change week numbers to month labels\n",
    "    ax[(i*2)+1].set_xticks([22, 31, 40])\n",
    "    ax[(i*2)+1].set_xticklabels(['June', 'Aug', 'Oct'])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_clusters.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90aeac-5d72-4fc5-be6d-80248909d712",
   "metadata": {},
   "source": [
    "### Annual trends stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34fc8d-8c8d-4a46-8966-5c6481953353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fig, ax = plt.subplots(2, 5, figsize=(14, 8))\n",
    "ax = ax.flatten()\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "cmap = plt.cm.viridis\n",
    "    \n",
    "# -----Iterate over subregions\n",
    "o1o2s = snowlines[['O1Region', 'O2Region']].drop_duplicates().dropna().sort_values(by=['O1Region', 'O2Region']).values\n",
    "ymin, ymax = -0.1, 1.1\n",
    "for i, (o1, o2) in enumerate(o1o2s):\n",
    "    # subset snowlines to subregion\n",
    "    snowlines_subregion = snowlines.loc[(snowlines['O1Region']==o1) & (snowlines['O2Region']==o2)]\n",
    "    if len(snowlines_subregion) < 1:\n",
    "        continue\n",
    "    subregion_name, color = snowlines_subregion[['Subregion', 'color']].values[0]\n",
    "    if color=='':\n",
    "        color='k'\n",
    "    # calculate moving median for all years separately\n",
    "    yearly_gb = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR']\n",
    "    yearly = yearly_gb.median().reset_index()\n",
    "    yearly.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    # plot\n",
    "    for year in yearly['Year'].drop_duplicates().values:\n",
    "        yearly_year = yearly.loc[yearly['Year']==year]\n",
    "        ax[i].plot(yearly_year['WOY'], yearly_year['AAR_median'], '-', color=cmap((year-2013)/(2023-2013)), linewidth=1)\n",
    "    ax[i].set_ylim(ymin, ymax)\n",
    "    ax[i].grid()\n",
    "    if (i==0) or (i==5):\n",
    "        ax[i].set_ylabel('AAR')\n",
    "    else:\n",
    "        ax[i].set_yticklabels([])\n",
    "    if i < 5:\n",
    "        ax[i].set_xticklabels([])\n",
    "    N = len(snowlines_subregion['site_name'].drop_duplicates())\n",
    "    ax[i].set_title(f'{subregion_name} (N={N})')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.94, 0.4, 0.01, 0.3])\n",
    "fig.colorbar(matplotlib.cm.ScalarMappable(norm=matplotlib.colors.Normalize(2013, 2023), cmap=cmap),\n",
    "             cax=cbar_ax, orientation='vertical')\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_years_stacked.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601bbd4-374c-459d-9694-301e7c6504fd",
   "metadata": {},
   "source": [
    "### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d503d69-12b4-4628-8a1b-7ed527415526",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -----Load ERA data\n",
    "era_fn = os.path.join(scm_path, 'all_ERA_data', 'all_era_data.csv')\n",
    "era = pd.read_csv(era_fn)\n",
    "era['Date'] = pd.to_datetime(era['Date'])\n",
    "# Add Year and WOY columns\n",
    "era['Year'] = era['Date'].dt.isocalendar().year\n",
    "era['WOY'] = era['Date'].dt.isocalendar().week\n",
    "\n",
    "# -----Load AOIs\n",
    "aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "\n",
    "# -----Add o1 and o2 region columns\n",
    "era[['O1Region', 'O2Region']] = '', ''\n",
    "for site_name in tqdm(era['site_name'].drop_duplicates().values):\n",
    "    aoi = aois.loc[aois['RGIId']==site_name]\n",
    "    o1, o2 = aoi[['O1Region', 'O2Region']].values[0]\n",
    "    era.loc[era['site_name']==site_name, 'O1Region'] = o1\n",
    "    era.loc[era['site_name']==site_name, 'O2Region'] = o2\n",
    "era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bea03-0cbe-48bc-97df-e39c11dfbbcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "plt.rcParams.update({'font.sans-serif':'Arial', 'font.size':12})\n",
    "fig, ax = plt.subplots(10, 2, figsize=(14, 35), gridspec_kw={'width_ratios':[4,1]})\n",
    "ax = ax.flatten()\n",
    "ymin1, ymax1 = 0, 1.05\n",
    "ymin2, ymax2 = 0, 1600\n",
    "ymin3, ymax3 = 0, 4\n",
    "\n",
    "weather_columns = ['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']\n",
    "colors = ['#f4a582', '#2166ac']\n",
    "q1, q3 = 0.25, 0.75\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "def calculate_weekly_trends(df, column):\n",
    "    ts_gb = df.groupby(by=['Year', 'WOY'])[column]\n",
    "    ts = ts_gb.median().reset_index()\n",
    "    ts = add_date_column(ts)\n",
    "    ts.rename(columns={column: column+'_median'}, inplace=True)\n",
    "    ts[column+'_Q1'] = ts_gb.quantile(q1).reset_index()[column]\n",
    "    ts[column+'_Q3'] = ts_gb.quantile(q3).reset_index()[column]\n",
    "    # calculate moving median for all years stacked\n",
    "    weekly_gb = df.groupby(by='WOY')[column]\n",
    "    weekly = weekly_gb.median().reset_index()\n",
    "    weekly.rename(columns={column: column+'_median'}, inplace=True)\n",
    "    weekly[column+'_Q1'] = weekly_gb.quantile(q1).reset_index()[column]\n",
    "    weekly[column+'_Q3'] = weekly_gb.quantile(q3).reset_index()[column]\n",
    "\n",
    "    return ts, weekly    \n",
    "\n",
    "# -----Iterate over subregions\n",
    "for i, (o1, o2) in enumerate(era[['O1Region', 'O2Region']].drop_duplicates().values):\n",
    "    \n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    print(subregion_name)\n",
    "    \n",
    "    # subset snowlines and ERA to subregion\n",
    "    era_subregion = era.loc[(era['O1Region']==o1) & (era['O2Region']==o2)]\n",
    "    snowlines_subregion = snowlines.loc[(snowlines['O1Region']==o1) & (snowlines['O2Region']==o2)]\n",
    "\n",
    "    # calculate moving medians over time and plot\n",
    "    # PDDs\n",
    "    column, color = weather_columns[0], colors[0]\n",
    "    ts, weekly = calculate_weekly_trends(era_subregion, column)\n",
    "    for year in ts['Year'].drop_duplicates().values + 1:\n",
    "        t1, t2 = pd.to_datetime(str(year-1)+'-11-01'), pd.to_datetime(str(year)+'-05-01')\n",
    "        ax[i*2].add_patch(Rectangle((t1, ymin1), width=t2-t1, height=ymax1-ymin1, \n",
    "                                    facecolor='#d9d9d9', edgecolor='None', zorder=1))\n",
    "    twin1 = ax[i*2].twinx()\n",
    "    twin1.fill_between(ts['date'], ts[column+'_median'], np.zeros(len(ts)), \n",
    "                       facecolor=color, alpha=0.5, zorder=2)\n",
    "    twin1.set_ylabel('$\\Sigma$PDDs', color=color)\n",
    "    twin1.spines['left'].set_color(color)\n",
    "    twin1.tick_params(axis='y', colors=color)\n",
    "    twin1.set_ylim(ymin2, ymax2)\n",
    "    # Snowfall\n",
    "    twin2 = ax[i*2].twinx()\n",
    "    column, color = weather_columns[1], colors[1]\n",
    "    ts, weekly = calculate_weekly_trends(era_subregion, column)\n",
    "    twin2.plot(ts['date'], ts[column+'_median'], '-', color=color, linewidth=2, zorder=3)\n",
    "    twin2.set_ylabel('$\\Sigma$Snowfall [m.w.e.]', color=color)\n",
    "    twin2.spines.right.set_color(color)\n",
    "    twin2.spines.right.set_position((\"axes\", 1.1))\n",
    "    twin2.tick_params(axis='y', colors=color)\n",
    "    twin2.set_ylim(ymin3, ymax3)\n",
    "    # AAR\n",
    "    column, color = 'AAR', 'k'\n",
    "    ts, weekly = calculate_weekly_trends(snowlines_subregion, column)\n",
    "    ax[i*2].set_ylim(ymin1, ymax1)\n",
    "    ax[i*2].plot(ts['date'], ts[column+'_median'], '.', color=color, markersize=7, zorder=5)\n",
    "    ax[i*2].set_ylabel('AAR', color=color)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly[column+'_Q1'], '-', linewidth=1, color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly[column+'_Q3'], '-', linewidth=1, color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly[column+'_median'], '-', linewidth=2, color=color)\n",
    "    ax[(i*2)+1].set_ylim(ymin1, ymax1)\n",
    "    # adjust axes\n",
    "    N = len(era_subregion['site_name'].drop_duplicates())\n",
    "    ax[i*2].set_title(f'{subregion_name} (N={N})')\n",
    "    ax[i*2].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    ax[i*2].set_xlim(np.datetime64('2013-01-01'), np.datetime64('2024-02-01'))\n",
    "    ax[(i*2)+1].grid()\n",
    "\n",
    "fig.subplots_adjust(hspace=0.25, wspace=0.4)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_pdd_snowfall.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-snow-cover-mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
