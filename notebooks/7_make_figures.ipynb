{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a268cb2-8765-4785-a416-f29c9289f371",
   "metadata": {},
   "source": [
    "# Make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886bd1-2d50-401f-aaef-8353f32db569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from shapely import wkt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import ast\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e6df1-03f5-472a-8c1c-222f8f92ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "# scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "aois_fn = os.path.join(scm_path, 'compiled_data', 'all_aois_climate_cluster.shp')\n",
    "eras_fn = os.path.join(scm_path, 'compiled_data', 'all_era_data.csv')\n",
    "clusters_fn = os.path.join(scm_path, 'compiled_data', 'climate_clusters.csv')\n",
    "snowlines_fn = os.path.join(scm_path, 'compiled_data', 'all_snowlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7489240-51f6-4b35-8c16-4927ccc6c615",
   "metadata": {},
   "source": [
    "## Define some colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c52145-2f0b-4149-8645-7e855d2118db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate clusters\n",
    "cluster_cmap_dict = {'Oceanic': '#1f78b4', \n",
    "                     'Continental': '#e31a1c',\n",
    "                     'Transitional-Continental': '#fb9a99',\n",
    "                     'Transitional-Temperate': '#b2df8a',\n",
    "                     'Temperate': '#33a02c'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032aa306-bb7e-4c94-ae71-74be17225a1b",
   "metadata": {},
   "source": [
    "## AAR time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d6a94-ee09-4d84-82e2-2b50558d1e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All glacier boundaries loaded from file.')\n",
    "\n",
    "# -----Load snowlines\n",
    "snowlines = pd.read_csv(snowlines_fn)\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "print('All snowlines loaded from file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba091c-ac8e-4ec1-ab11-9c43bcf65934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Add Year and WOY columns to snowlines\n",
    "snowlines['Year'] = snowlines['datetime'].dt.isocalendar().year\n",
    "snowlines['WOY'] = snowlines['datetime'].dt.isocalendar().week\n",
    "\n",
    "# -----Add subregion and climate cluster columns to snowlines\n",
    "snowlines[['Subregion', 'color']] = '', '', \n",
    "for o1, o2 in tqdm(aois[['O1Region', 'O2Region']].drop_duplicates().values):\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1) & (aois['O2Region']==o2)]\n",
    "    site_names = aois_subregion['RGIId'].drop_duplicates().values\n",
    "    for site_name in site_names:\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'O1Region'] = o1\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'O2Region'] = o2\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'Subregion'] = subregion_name\n",
    "        snowlines.loc[snowlines['site_name']==site_name, 'color'] = color\n",
    "snowlines[['cluster', 'clustName']] = '', '', \n",
    "for site_name in (climate_clusters['site_name'].drop_duplicates().values):\n",
    "    climate_clusters_site = climate_clusters.loc[climate_clusters['site_name']==site_name]\n",
    "    snowlines.loc[snowlines['site_name']==site_name, 'cluster'] = climate_clusters_site['cluster'].values[0]\n",
    "    snowlines.loc[snowlines['site_name']==site_name, 'clustName'] = climate_clusters_site['clustName'].values[0]\n",
    "snowlines = snowlines.loc[snowlines['cluster']!='']\n",
    "snowlines.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bbf70-58ae-4d5c-b0c3-57dc8fe5b3fa",
   "metadata": {},
   "source": [
    "### By subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bcfef-fecc-4a12-8f64-5f5b971f871b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fig, ax = plt.subplots(10, 2, figsize=(12, 40), gridspec_kw={'width_ratios':[4,1]})\n",
    "ax = ax.flatten()\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "q1, q3 = 0.25, 0.75\n",
    "\n",
    "# -----Iterate over subregions\n",
    "o1o2s = snowlines[['O1Region', 'O2Region']].drop_duplicates().dropna().sort_values(by=['O1Region', 'O2Region']).values\n",
    "ymin, ymax = -0.1, 1.1\n",
    "for i, (o1, o2) in enumerate(o1o2s):\n",
    "    # subset snowlines to subregion\n",
    "    snowlines_subregion = snowlines.loc[(snowlines['O1Region']==o1) & (snowlines['O2Region']==o2)]\n",
    "    if len(snowlines_subregion) < 1:\n",
    "        continue\n",
    "    subregion_name, color = snowlines_subregion[['Subregion', 'color']].values[0]\n",
    "    if color=='':\n",
    "        color='k'\n",
    "    # calculate moving median over time\n",
    "    ts_gb = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR']\n",
    "    ts = ts_gb.median().reset_index()\n",
    "    ts = add_date_column(ts)\n",
    "    ts.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    ts['AAR_Q1'] = ts_gb.quantile(q1).reset_index()['AAR']\n",
    "    ts['AAR_Q3'] = ts_gb.quantile(q3).reset_index()['AAR']\n",
    "    # calculate moving median for all years stacked\n",
    "    weekly_gb = snowlines_subregion.groupby(by='WOY')['AAR']\n",
    "    weekly = weekly_gb.median().reset_index()\n",
    "    weekly.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    weekly['AAR_Q1'] = weekly_gb.quantile(q1).reset_index()['AAR']\n",
    "    weekly['AAR_Q3'] = weekly_gb.quantile(q3).reset_index()['AAR']\n",
    "    weekly['WOY'] = weekly['WOY'].values.astype(float)\n",
    "    # plot\n",
    "    # add grey boxes where observations don't exist\n",
    "    for year in ts['Year'].values:\n",
    "        t1, t2 = pd.to_datetime(str(year-1)+'-11-01'), pd.to_datetime(str(year)+'-05-01')\n",
    "        ax[i*2].add_patch(Rectangle((t1, ymin), width=t2-t1, height=ymax-ymin, facecolor='#d9d9d9', edgecolor='None'))\n",
    "        ts_year = ts.loc[ts['Year']==year]\n",
    "        ax[i*2].plot(ts_year['date'], ts_year['AAR_median'], 'o', color=color, linewidth=0.5, \n",
    "                     markerfacecolor=color, markeredgecolor='w', markersize=5)\n",
    "    ax[i*2].set_ylim(ymin, ymax)\n",
    "    ax[i*2].grid()\n",
    "    ax[i*2].set_ylabel('AAR')\n",
    "    N = len(snowlines_subregion['site_name'].drop_duplicates())\n",
    "    ax[i*2].set_title(f'{subregion_name} (N={N})')\n",
    "    ax[i*2].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    ax[(i*2)+1].set_ylim(ymin, ymax)\n",
    "    ax[(i*2)+1].grid()\n",
    "    ax[(i*2)+1].fill_between(weekly['WOY'], weekly['AAR_Q1'], weekly['AAR_Q3'], color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly['AAR_median'], '-', linewidth=2, color=color)\n",
    "    # change week numbers to month labels\n",
    "    ax[(i*2)+1].set_xticks([22, 31, 40])\n",
    "    ax[(i*2)+1].set_xticklabels(['June', 'Aug', 'Oct'])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_subregions.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a405b-9c21-49ed-9ddb-db00161db02c",
   "metadata": {},
   "source": [
    "### By climate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57b313-26ef-485a-a2e8-f61b6a6164b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "n_clusters = len(snowlines['cluster'].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(n_clusters, 2, figsize=(12, 4*n_clusters), gridspec_kw={'width_ratios':[4,1]})\n",
    "ax = ax.flatten()\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "q1, q3 = 0.25, 0.75\n",
    "\n",
    "# -----Iterate over subregions\n",
    "ymin, ymax = -0.1, 1.1\n",
    "for i, cluster_name in enumerate(['Oceanic', \n",
    "                                  'Continental', \n",
    "                                  'Transitional-Continental', \n",
    "                                  'Transitional-Temperate', \n",
    "                                  'Temperate']):\n",
    "    # subset snowlines to cluster\n",
    "    snowlines_cluster = snowlines.loc[snowlines['clustName']==cluster_name]\n",
    "    if len(snowlines_cluster) < 1:\n",
    "        continue\n",
    "    # grab cluster name\n",
    "    color = cluster_cmap_dict[cluster_name]\n",
    "    # calculate moving median over time\n",
    "    ts_gb = snowlines_cluster.groupby(by=['Year', 'WOY'])['AAR']\n",
    "    ts = ts_gb.median().reset_index()\n",
    "    ts = add_date_column(ts)\n",
    "    ts.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    ts['AAR_Q1'] = ts_gb.quantile(q1).reset_index()['AAR']\n",
    "    ts['AAR_Q3'] = ts_gb.quantile(q3).reset_index()['AAR']\n",
    "    # calculate moving median for all years stacked\n",
    "    weekly_gb = snowlines_cluster.groupby(by='WOY')['AAR']\n",
    "    weekly = weekly_gb.median().reset_index()\n",
    "    weekly.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    weekly['AAR_Q1'] = weekly_gb.quantile(q1).reset_index()['AAR']\n",
    "    weekly['AAR_Q3'] = weekly_gb.quantile(q3).reset_index()['AAR']\n",
    "    weekly['WOY'] = weekly['WOY'].values.astype(float)\n",
    "    # plot\n",
    "    # add grey boxes where observations don't exist\n",
    "    for year in ts['Year'].values:\n",
    "        t1, t2 = pd.to_datetime(str(year-1)+'-11-01'), pd.to_datetime(str(year)+'-05-01')\n",
    "        ax[i*2].add_patch(Rectangle((t1, ymin), width=t2-t1, height=ymax-ymin, facecolor='#d9d9d9', edgecolor='None'))\n",
    "        ts_year = ts.loc[ts['Year']==year]\n",
    "        ax[i*2].plot(ts_year['date'], ts_year['AAR_median'], 'o', color=color, linewidth=0.5, \n",
    "                     markerfacecolor=color, markeredgecolor='w', markersize=5)\n",
    "    ax[i*2].set_ylim(ymin, ymax)\n",
    "    ax[i*2].grid()\n",
    "    ax[i*2].set_ylabel('AAR')\n",
    "    N = len(snowlines_cluster['site_name'].drop_duplicates())\n",
    "    ax[i*2].set_title(f'{cluster_name} (N={N})')\n",
    "    ax[i*2].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    ax[(i*2)+1].set_ylim(ymin, ymax)\n",
    "    ax[(i*2)+1].grid()\n",
    "    ax[(i*2)+1].fill_between(weekly['WOY'], weekly['AAR_Q1'], weekly['AAR_Q3'], color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly['AAR_median'], '-', linewidth=2, color=color)\n",
    "    # change week numbers to month labels\n",
    "    ax[(i*2)+1].set_xticks([22, 31, 40])\n",
    "    ax[(i*2)+1].set_xticklabels(['June', 'Aug', 'Oct'])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_clusters.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90aeac-5d72-4fc5-be6d-80248909d712",
   "metadata": {},
   "source": [
    "### Annual trends stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34fc8d-8c8d-4a46-8966-5c6481953353",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fig, ax = plt.subplots(2, 5, figsize=(14, 8))\n",
    "ax = ax.flatten()\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "cmap = plt.cm.viridis\n",
    "    \n",
    "# -----Iterate over subregions\n",
    "o1o2s = snowlines[['O1Region', 'O2Region']].drop_duplicates().dropna().sort_values(by=['O1Region', 'O2Region']).values\n",
    "ymin, ymax = -0.1, 1.1\n",
    "for i, (o1, o2) in enumerate(o1o2s):\n",
    "    # subset snowlines to subregion\n",
    "    snowlines_subregion = snowlines.loc[(snowlines['O1Region']==o1) & (snowlines['O2Region']==o2)]\n",
    "    if len(snowlines_subregion) < 1:\n",
    "        continue\n",
    "    subregion_name, color = snowlines_subregion[['Subregion', 'color']].values[0]\n",
    "    if color=='':\n",
    "        color='k'\n",
    "    # calculate moving median for all years separately\n",
    "    yearly_gb = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR']\n",
    "    yearly = yearly_gb.median().reset_index()\n",
    "    yearly.rename(columns={'AAR': 'AAR_median'}, inplace=True)\n",
    "    # plot\n",
    "    for year in yearly['Year'].drop_duplicates().values:\n",
    "        yearly_year = yearly.loc[yearly['Year']==year]\n",
    "        ax[i].plot(yearly_year['WOY'], yearly_year['AAR_median'], '-', color=cmap((year-2013)/(2023-2013)), linewidth=1)\n",
    "    ax[i].set_ylim(ymin, ymax)\n",
    "    ax[i].grid()\n",
    "    if (i==0) or (i==5):\n",
    "        ax[i].set_ylabel('AAR')\n",
    "    else:\n",
    "        ax[i].set_yticklabels([])\n",
    "    if i < 5:\n",
    "        ax[i].set_xticklabels([])\n",
    "    N = len(snowlines_subregion['site_name'].drop_duplicates())\n",
    "    ax[i].set_title(f'{subregion_name} (N={N})')\n",
    "\n",
    "cbar_ax = fig.add_axes([0.94, 0.4, 0.01, 0.3])\n",
    "fig.colorbar(matplotlib.cm.ScalarMappable(norm=matplotlib.colors.Normalize(2013, 2023), cmap=cmap),\n",
    "             cax=cbar_ax, orientation='vertical')\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_years_stacked.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601bbd4-374c-459d-9694-301e7c6504fd",
   "metadata": {},
   "source": [
    "### Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d503d69-12b4-4628-8a1b-7ed527415526",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -----Load ERA data\n",
    "era_fn = os.path.join(scm_path, 'all_ERA_data', 'all_era_data.csv')\n",
    "era = pd.read_csv(era_fn)\n",
    "era['Date'] = pd.to_datetime(era['Date'])\n",
    "# Add Year and WOY columns\n",
    "era['Year'] = era['Date'].dt.isocalendar().year\n",
    "era['WOY'] = era['Date'].dt.isocalendar().week\n",
    "\n",
    "# -----Load AOIs\n",
    "aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "\n",
    "# -----Add o1 and o2 region columns\n",
    "era[['O1Region', 'O2Region']] = '', ''\n",
    "for site_name in tqdm(era['site_name'].drop_duplicates().values):\n",
    "    aoi = aois.loc[aois['RGIId']==site_name]\n",
    "    o1, o2 = aoi[['O1Region', 'O2Region']].values[0]\n",
    "    era.loc[era['site_name']==site_name, 'O1Region'] = o1\n",
    "    era.loc[era['site_name']==site_name, 'O2Region'] = o2\n",
    "era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bea03-0cbe-48bc-97df-e39c11dfbbcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "plt.rcParams.update({'font.sans-serif':'Arial', 'font.size':12})\n",
    "fig, ax = plt.subplots(10, 2, figsize=(14, 35), gridspec_kw={'width_ratios':[4,1]})\n",
    "ax = ax.flatten()\n",
    "ymin1, ymax1 = 0, 1.05\n",
    "ymin2, ymax2 = 0, 1600\n",
    "ymin3, ymax3 = 0, 4\n",
    "\n",
    "weather_columns = ['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']\n",
    "colors = ['#f4a582', '#2166ac']\n",
    "q1, q3 = 0.25, 0.75\n",
    "\n",
    "def add_date_column(df):\n",
    "    df['date'] = pd.to_datetime(df['Year'].astype(str) + df['WOY'].astype(str) + '1', format='%Y%W%w')\n",
    "    df['date'] = df['date'] - pd.Timedelta(days=1)\n",
    "    return df\n",
    "\n",
    "def calculate_weekly_trends(df, column):\n",
    "    ts_gb = df.groupby(by=['Year', 'WOY'])[column]\n",
    "    ts = ts_gb.median().reset_index()\n",
    "    ts = add_date_column(ts)\n",
    "    ts.rename(columns={column: column+'_median'}, inplace=True)\n",
    "    ts[column+'_Q1'] = ts_gb.quantile(q1).reset_index()[column]\n",
    "    ts[column+'_Q3'] = ts_gb.quantile(q3).reset_index()[column]\n",
    "    # calculate moving median for all years stacked\n",
    "    weekly_gb = df.groupby(by='WOY')[column]\n",
    "    weekly = weekly_gb.median().reset_index()\n",
    "    weekly.rename(columns={column: column+'_median'}, inplace=True)\n",
    "    weekly[column+'_Q1'] = weekly_gb.quantile(q1).reset_index()[column]\n",
    "    weekly[column+'_Q3'] = weekly_gb.quantile(q3).reset_index()[column]\n",
    "\n",
    "    return ts, weekly    \n",
    "\n",
    "# -----Iterate over subregions\n",
    "for i, (o1, o2) in enumerate(era[['O1Region', 'O2Region']].drop_duplicates().values):\n",
    "    \n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    print(subregion_name)\n",
    "    \n",
    "    # subset snowlines and ERA to subregion\n",
    "    era_subregion = era.loc[(era['O1Region']==o1) & (era['O2Region']==o2)]\n",
    "    snowlines_subregion = snowlines.loc[(snowlines['O1Region']==o1) & (snowlines['O2Region']==o2)]\n",
    "\n",
    "    # calculate moving medians over time and plot\n",
    "    # PDDs\n",
    "    column, color = weather_columns[0], colors[0]\n",
    "    ts, weekly = calculate_weekly_trends(era_subregion, column)\n",
    "    for year in ts['Year'].drop_duplicates().values + 1:\n",
    "        t1, t2 = pd.to_datetime(str(year-1)+'-11-01'), pd.to_datetime(str(year)+'-05-01')\n",
    "        ax[i*2].add_patch(Rectangle((t1, ymin1), width=t2-t1, height=ymax1-ymin1, \n",
    "                                    facecolor='#d9d9d9', edgecolor='None', zorder=1))\n",
    "    twin1 = ax[i*2].twinx()\n",
    "    twin1.fill_between(ts['date'], ts[column+'_median'], np.zeros(len(ts)), \n",
    "                       facecolor=color, alpha=0.5, zorder=2)\n",
    "    twin1.set_ylabel('$\\Sigma$PDDs', color=color)\n",
    "    twin1.spines['left'].set_color(color)\n",
    "    twin1.tick_params(axis='y', colors=color)\n",
    "    twin1.set_ylim(ymin2, ymax2)\n",
    "    # Snowfall\n",
    "    twin2 = ax[i*2].twinx()\n",
    "    column, color = weather_columns[1], colors[1]\n",
    "    ts, weekly = calculate_weekly_trends(era_subregion, column)\n",
    "    twin2.plot(ts['date'], ts[column+'_median'], '-', color=color, linewidth=2, zorder=3)\n",
    "    twin2.set_ylabel('$\\Sigma$Snowfall [m.w.e.]', color=color)\n",
    "    twin2.spines.right.set_color(color)\n",
    "    twin2.spines.right.set_position((\"axes\", 1.1))\n",
    "    twin2.tick_params(axis='y', colors=color)\n",
    "    twin2.set_ylim(ymin3, ymax3)\n",
    "    # AAR\n",
    "    column, color = 'AAR', 'k'\n",
    "    ts, weekly = calculate_weekly_trends(snowlines_subregion, column)\n",
    "    ax[i*2].set_ylim(ymin1, ymax1)\n",
    "    ax[i*2].plot(ts['date'], ts[column+'_median'], '.', color=color, markersize=7, zorder=5)\n",
    "    ax[i*2].set_ylabel('AAR', color=color)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly[column+'_Q1'], '-', linewidth=1, color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly[column+'_Q3'], '-', linewidth=1, color=color, alpha=0.5)\n",
    "    ax[(i*2)+1].plot(weekly['WOY'], weekly[column+'_median'], '-', linewidth=2, color=color)\n",
    "    ax[(i*2)+1].set_ylim(ymin1, ymax1)\n",
    "    # adjust axes\n",
    "    N = len(era_subregion['site_name'].drop_duplicates())\n",
    "    ax[i*2].set_title(f'{subregion_name} (N={N})')\n",
    "    ax[i*2].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "    ax[i*2].set_xlim(np.datetime64('2013-01-01'), np.datetime64('2024-02-01'))\n",
    "    ax[(i*2)+1].grid()\n",
    "\n",
    "fig.subplots_adjust(hspace=0.25, wspace=0.4)\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'timeseries_aar_pdd_snowfall.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e202626-5344-42af-8409-b6926e4c8118",
   "metadata": {},
   "source": [
    "## Median AARs with different characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18a2fa-da10-4d01-837d-259aa1602fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load minimum snow cover characteristics\n",
    "min_snow_cover_stats_fn = os.path.join(scm_path, 'results', 'min_snow_cover_stats.csv')\n",
    "min_snow_cover_stats = pd.read_csv(min_snow_cover_stats_fn)\n",
    "min_snow_cover_stats['geometry'] = min_snow_cover_stats['geometry'].apply(wkt.loads)\n",
    "# sort by region numbers\n",
    "min_snow_cover_stats.sort_values(by=['O1Region', 'O2Region'], inplace=True)\n",
    "# add cluster column\n",
    "min_snow_cover_stats[['cluster', 'clustName']] = '', ''\n",
    "for site_name in min_snow_cover_stats['RGIId'].drop_duplicates().values:\n",
    "    clusters_site = clusters.loc[clusters['site_name']==site_name]\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==site_name, 'cluster'] = clusters_site['cluster'].values[0]\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==site_name, 'clustName'] = clusters_site['clustName'].values[0]\n",
    "\n",
    "# add glacier boundary x and y centroid coordinates for plotting\n",
    "min_snow_cover_stats['centroid_x'] = [x.centroid.coords.xy[0][0] for x in min_snow_cover_stats['geometry']]\n",
    "min_snow_cover_stats['centroid_y'] = [x.centroid.coords.xy[1][0] for x in min_snow_cover_stats['geometry']]\n",
    "\n",
    "# -----Load country outlines for plotting\n",
    "countries_fn = os.path.join(scm_path, '..', 'GIS_data', 'countries_shp', 'countries.shp')\n",
    "countries = gpd.read_file(countries_fn)\n",
    "usca = countries.loc[(countries['NAME']=='United States') | (countries['NAME']=='Canada')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8731d23-5255-445b-a22e-7987439647d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----Plot maps of minimum AARs by various columns\n",
    "columns = ['Subregion', 'clustName', 'Aspect', 'Zmed', 'Area', 'Slope']\n",
    "columns_display = ['Subregion', 'Climate cluster', 'Aspect [degrees]', 'Median elevation [m]', 'Area [km$^2$]', 'Slope [degrees]']\n",
    "palettes = [dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values),\n",
    "            cluster_cmap_dict,\n",
    "            'hls', \n",
    "            'mako', \n",
    "            'viridis', \n",
    "            'crest']\n",
    "bins_list = ['N/A', \n",
    "             'N/A',\n",
    "             np.linspace(0,360, num=7), \n",
    "             np.linspace(500, 3000, num=6),\n",
    "             np.linspace(0, 750, num=7),\n",
    "             np.linspace(0, 35, num=8)]\n",
    "plt.rcParams.update({'font.sans-serif':'Arial', 'font.size':12})\n",
    "xmin, xmax = -168, -112\n",
    "ymin, ymax = 45, 65\n",
    "for column, column_display, palette, bins in zip(columns, columns_display, palettes, bins_list):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10,10), gridspec_kw={'height_ratios':[2.5, 1]})\n",
    "    # plot country outlines on the map\n",
    "    usca.plot(ax=ax[0], facecolor='None', edgecolor='grey')\n",
    "    if column=='Subregion':\n",
    "        # plot points on the map\n",
    "        scatter = sns.scatterplot(data=min_snow_cover_stats, x='centroid_x', y='centroid_y', \n",
    "                                  hue=column, palette=palette, size='AAR_P50_min', \n",
    "                                  sizes=(5, 100), legend=True, ax=ax[0])\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=min_snow_cover_stats, x=column, y='AAR_P50_min', showfliers=False, palette=palette, ax=ax[1])\n",
    "        ax[1].set_xticks(ax[1].get_xticks())\n",
    "        ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "        # calculate number of obserations in each bin\n",
    "        nobs = min_snow_cover_stats['Subregion'].value_counts().values\n",
    "    elif column=='clustName':\n",
    "        # plot points on the map\n",
    "        scatter = sns.scatterplot(data=min_snow_cover_stats, x='centroid_x', y='centroid_y', \n",
    "                                  hue=column, palette=palette, size='AAR_P50_min', \n",
    "                                  sizes=(5, 100), legend=True, ax=ax[0])\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=min_snow_cover_stats, x=column, y='AAR_P50_min', showfliers=False, palette=palette, ax=ax[1])\n",
    "        ax[1].set_xticks(ax[1].get_xticks())\n",
    "        ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "        # calculate number of obserations in each bin\n",
    "        nobs = min_snow_cover_stats['cluster'].value_counts().values\n",
    "    else:\n",
    "        # plot points on the map\n",
    "        sns.scatterplot(data=min_snow_cover_stats, x='centroid_x', y='centroid_y', \n",
    "                        hue=column, size='AAR_P50_min', palette=palette,\n",
    "                        sizes=(5, 100), legend=True, ax=ax[0])\n",
    "        # add bin column\n",
    "        min_snow_cover_stats[column + '_bin'] = pd.cut(min_snow_cover_stats[column], bins)\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=min_snow_cover_stats, x=column + '_bin', y='AAR_P50_min', showfliers=False, palette=palette, ax=ax[1])\n",
    "        # calculate number of observations in each bin\n",
    "        nobs = min_snow_cover_stats[column + '_bin'].value_counts().values\n",
    "    # Add number of observations to the boxplot\n",
    "    nobs = [str(x) for x in nobs.tolist()]\n",
    "    pos = range(len(nobs))\n",
    "    for tick in pos:\n",
    "        ax[1].text(pos[tick], 1.0,\n",
    "                   nobs[tick],\n",
    "                   horizontalalignment='center',\n",
    "                   color='k')\n",
    "    # adjust legend and axes\n",
    "    sns.move_legend(ax[0], \"center right\", bbox_to_anchor=[1.1, 0.4, 0.2, 0.2])\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].set_ylabel('')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlim(xmin, xmax)\n",
    "    ax[0].set_ylim(ymin, ymax)\n",
    "    ax[1].set_xlabel(column_display)\n",
    "    ax[1].set_ylim(0, 0.95)\n",
    "    ax[1].set_ylabel('AAR')\n",
    "\n",
    "    # Edit legend labels for map\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    labels = [x.replace('AAR_P50_min', 'Median AAR').replace(column, column_display) for x in labels]\n",
    "    ax[0].legend(handles, labels, loc='center right')\n",
    "    if column=='clustName':\n",
    "        anchor = [1.15, 0.4, 0.2, 0.2]\n",
    "    else: \n",
    "        anchor = [1.1, 0.4, 0.2, 0.2]\n",
    "    sns.move_legend(ax[0], \"center right\", bbox_to_anchor=anchor)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_fn = 'aars_map_' + column + '.png'\n",
    "    fig.savefig(os.path.join(figures_out_path, fig_fn), dpi=250, bbox_inches='tight')\n",
    "    print('figure saved to file: ', os.path.join(figures_out_path, fig_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a15c0-96d6-4f57-8eea-5554eb979edd",
   "metadata": {},
   "source": [
    "## AAR-PDD linear fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211caba-08da-4ae9-8f50-7d9e2e297fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AAR-PDD linear fits\n",
    "aar_pdd_linear_fn = os.path.join(scm_path, 'results', 'aar_pdd_snowfall_linear_fit_sites.csv')\n",
    "aar_pdd_linear_df = pd.read_csv(aar_pdd_linear_fn)\n",
    "# Add subregion name and color\n",
    "aar_pdd_linear_df[['Subregion', 'color']] = '', ''\n",
    "for o1, o2 in aar_pdd_linear_df[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    aar_pdd_linear_df.loc[(aar_pdd_linear_df['O1Region']==o1) & (aar_pdd_linear_df['O2Region']==o2), 'Subregion'] = subregion_name\n",
    "    aar_pdd_linear_df.loc[(aar_pdd_linear_df['O1Region']==o1) & (aar_pdd_linear_df['O2Region']==o2), 'color'] = color\n",
    "\n",
    "aar_pdd_linear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733aa3ef-4f05-4921-83a9-8e6e8fbeabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,6), sharey=True)\n",
    "# by climate cluster\n",
    "sns.boxplot(data=aar_pdd_linear_df, x='clustName', y='coef_linear', ax=ax[0], \n",
    "            palette=cluster_cmap_dict, showfliers=False)\n",
    "# by subregion\n",
    "sns.boxplot(data=aar_pdd_linear_df, x='Subregion', y='coef_linear', ax=ax[1],\n",
    "           palette=dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values), showfliers=False)\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "ax[0].set_ylabel('AAR vs. PDD linear fit coefficient')\n",
    "ax[1].set_ylabel('')\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "fig.subplots_adjust(wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b03a1-9492-4dbf-86c3-11047c64132c",
   "metadata": {},
   "source": [
    "## Distribution of climates in each subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9072697-b33f-43aa-8b06-e7d028067968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subregion names to aois\n",
    "for o1, o2 in aois[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    aois.loc[(aois['O1Region']==o1) & (aois['O2Region']==o2), 'Subregion'] = subregion_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f87447-39e2-4b39-b783-6c31c4ce38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group AOIs by Subregion and cluster to count values\n",
    "grouped = aois.groupby(['Subregion', 'clustName']).size().unstack(fill_value=0)\n",
    "# sort columns and indices\n",
    "cluster_order = ['Oceanic', 'Continental', 'Transitional-Continental', 'Transitional-Temperate', 'Temperate']\n",
    "grouped = grouped[cluster_order]\n",
    "subregions_order = ['Aleutians', 'Alaska Range', 'W. Chugach Mtns.', \n",
    "                    'St. Elias Mtns.', 'N. Rockies', 'N. Coast Ranges', \n",
    "                    'N. Cascades', 'C. Rockies', 'S. Cascades', 'S. Rockies']\n",
    "grouped = grouped.reindex(subregions_order)\n",
    "# subset into roughly coastal and not coastal groups\n",
    "group1 = ['S. Cascades', 'N. Cascades', 'W. Chugach Mtns.', 'N. Coast Ranges', 'Aleutians']\n",
    "grouped1 = grouped.loc[group1]\n",
    "group2 = ['S. Rockies', 'C. Rockies', 'N. Rockies', 'St. Elias Mtns.', 'Alaska Range']\n",
    "grouped2 = grouped.loc[group2]\n",
    "\n",
    "# Plot stacked bar plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "grouped1.plot(kind='barh', stacked=True, legend=True,\n",
    "              color=[cluster_cmap_dict.get(x, 'black') for x in grouped.columns], \n",
    "              ax=ax[0])\n",
    "grouped2.plot(kind='barh', stacked=True, legend=False,\n",
    "              color=[cluster_cmap_dict.get(x, 'black') for x in grouped.columns], \n",
    "              ax=ax[1])\n",
    "ax[0].set_xlabel('Counts')\n",
    "ax[1].set_xlabel('Counts')\n",
    "ax[1].set_ylabel('')\n",
    "fig.suptitle('Distribution of clusters within each Subregion')\n",
    "# adjust legend\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].get_legend().remove()\n",
    "fig.subplots_adjust(wspace=0.35)\n",
    "fig.legend(handles, labels, title='Cluster', loc='upper right', bbox_to_anchor=[0.93, 0.4, 0.2, 0.2])\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'subregions_clusters_distribution.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7ec6c-dc79-4c0c-b676-5c11b42e31a7",
   "metadata": {},
   "source": [
    "## Plot linear changes in AARs for 2016-2023 by terrain characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b33bf-506e-4112-b171-9c69e5100211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load AARs linear fit\n",
    "aars_linear_fit_fn = 'minimum_AARs_linear_fit.csv'\n",
    "aars_linear_fit = pd.read_csv(os.path.join(snowlines_path, aars_linear_fit_fn))\n",
    "\n",
    "# -----Add glacier terrain and geometry columns\n",
    "columns = ['geometry', 'centroid_x', 'centroid_y', 'O1Region', 'O2Region', 'Subregion', 'color', 'Aspect', 'Slope', 'Zmed', 'Area']\n",
    "for column in columns:\n",
    "    aars_linear_fit[column] = np.nan\n",
    "# iterate over site names\n",
    "for site_name in aars_linear_fit['site_name'].drop_duplicates().values:\n",
    "    min_snow_cover_stats_site = min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==site_name]\n",
    "    try:\n",
    "        for column in columns:\n",
    "            aars_linear_fit.loc[aars_linear_fit['site_name']==site_name, column] = [min_snow_cover_stats_site[column].values[0]]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "aars_linear_fit.dropna(inplace=True)\n",
    "\n",
    "# -----Add column for negative value of change category to show more negative values as larger markers\n",
    "aars_linear_fit['linear_fit_coef_neg'] = - aars_linear_fit['linear_fit_coef']\n",
    "# Sort by region numbers\n",
    "aars_linear_fit.sort_values(by=['O1Region', 'O2Region'], inplace=True)\n",
    "aars_linear_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703f590-0e6d-47a1-87ff-64543942cf04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Plot map of AARs linear fit with various columns\n",
    "columns = ['Subregion', 'Aspect', 'Zmed', 'Area', 'Slope']\n",
    "columns_display = ['Subregion', 'Aspect [degrees]', 'Median elevation [m]', 'Area [km$^2$]', 'Slope [degrees]']\n",
    "palettes = [dict(aars_linear_fit[['Subregion', 'color']].drop_duplicates().values),\n",
    "            'hls', \n",
    "            'mako', \n",
    "            'viridis', \n",
    "            'crest']\n",
    "bins_list = ['N/A', \n",
    "             np.linspace(0,360, num=7), \n",
    "             np.linspace(500, 3000, num=6),\n",
    "             np.linspace(0, 750, num=7),\n",
    "             np.linspace(0, 35, num=8)]\n",
    "plt.rcParams.update({'font.sans-serif':'Arial', 'font.size':12})\n",
    "xmin, xmax = -168, -112\n",
    "ymin, ymax = 45, 65\n",
    "def map_size(value):\n",
    "    return np.abs(value) * 100\n",
    "for column, column_display, palette, bins in zip(columns, columns_display, palettes, bins_list):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10,10), gridspec_kw={'height_ratios':[2.5, 1]})\n",
    "    # plot country outlines on the map\n",
    "    usca.plot(ax=ax[0], facecolor='None', edgecolor='grey')\n",
    "    # plot points on the map\n",
    "    if type(bins) != str:\n",
    "        sns.scatterplot(data=aars_linear_fit, x='centroid_x', y='centroid_y', \n",
    "                        hue=column, palette=palette, size='linear_fit_coef_neg', sizes=(5, 150), legend=True, ax=ax[0])\n",
    "    else:\n",
    "        sns.scatterplot(data=aars_linear_fit, x='centroid_x', y='centroid_y', \n",
    "                        hue=column, palette=palette, size='linear_fit_coef_neg', sizes=(5, 150), legend=True, ax=ax[0])\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].set_ylabel('')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlim(xmin, xmax)\n",
    "    ax[0].set_ylim(ymin, ymax)\n",
    "    # plot regional barplots\n",
    "    if type(bins) != str:\n",
    "        # add bin column\n",
    "        aars_linear_fit[column + '_bin'] = pd.cut(aars_linear_fit[column], bins)\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=aars_linear_fit, x=column + '_bin', y='linear_fit_coef', showfliers=False, palette=palette, ax=ax[1])\n",
    "    else:\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=aars_linear_fit, x=column, y='linear_fit_coef', showfliers=False, palette=palette, ax=ax[1])\n",
    "    if type(bins) == str:\n",
    "        ax[1].set_xticks(ax[1].get_xticks())\n",
    "        ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "    ax[1].set_xlabel(column_display)\n",
    "    ax[1].set_ylabel('AAR linear trend [per year]')\n",
    "    \n",
    "    # Edit legend labels for map\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    labels = [x.replace('linear_fit_coef_neg', 'AAR linear trend [per year]').replace(column, column_display) for x in labels]\n",
    "    labels = [x.replace('0.', '-0.').replace('âˆ’-0', '0') for x in labels]\n",
    "    ax[0].legend(handles, labels, loc='center right')\n",
    "    sns.move_legend(ax[0], \"center right\", bbox_to_anchor=[1.15, 0.4, 0.2, 0.2])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save figure\n",
    "    fig_fn = 'AARs_linear_fit_map_' + column + '.png'\n",
    "    fig.savefig(os.path.join(figures_out_path, fig_fn), dpi=250, bbox_inches='tight')\n",
    "    print('figure saved to file: ', os.path.join(figures_out_path, fig_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b68ebc-9738-402a-be93-00ec03a03188",
   "metadata": {},
   "source": [
    "## Plot AAR and linear fit time series for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d862d-aba2-4425-9f6f-fcac82f11a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load snowlines\n",
    "snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "# Load minimum AAR fits\n",
    "min_aars_fn = 'minimum_AARs_linear_fit.csv'\n",
    "min_aars = pd.read_csv(os.path.join(snowlines_path, min_aars_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd656bc3-3b94-4120-8fd3-4629c3414e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over sites\n",
    "for site_name in tqdm(min_aars['site_name'].drop_duplicates().values):\n",
    "    # Grab snowlines\n",
    "    snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "    # Grab min AARs\n",
    "    min_aars_site = min_aars.loc[min_aars['site_name']==site_name].reset_index(drop=True)\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    ax.plot(snowlines_site['datetime'], snowlines_site['AAR'], '.k', markersize=1)\n",
    "    ax.plot(min_aars_site['minimum_AARs_dts'], min_aars_site['minimum_AARs'], '*m', markersize=2)\n",
    "    ax.grid()\n",
    "    ax.set_title(site_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362f05b-eeea-4d4a-8230-48acd61f31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.literal_eval(min_aars['minimum_AARs'][0])#.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c27fc6-44b4-4ef4-865d-d5d5f01882b7",
   "metadata": {},
   "source": [
    "## Plot AAR and PDD time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2134d-7570-4fc9-9a34-30034c9bb982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load all snowlines\n",
    "snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "\n",
    "# -----Load all ERA data\n",
    "eras = pd.read_csv(os.path.join(eras_path, eras_fn))\n",
    "eras['Date'] = pd.to_datetime(eras['Date'], format='mixed')\n",
    "\n",
    "# -----Load all AOIs\n",
    "aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f1189-9eef-4dfc-8a76-a5066d748d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Plot median trends for each subregion\n",
    "# add year and WOY columns to snowlines and ERA\n",
    "snowlines['Year'] = snowlines['datetime'].dt.isocalendar().year\n",
    "snowlines['WOY'] = snowlines['datetime'].dt.isocalendar().week\n",
    "eras['Year'] = eras['Date'].dt.isocalendar().year\n",
    "eras['WOY'] = eras['Date'].dt.isocalendar().week\n",
    "# grab all subregions\n",
    "subregions = aois[['O1Region', 'O2Region']].drop_duplicates().values\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots(len(subregions), 1, figsize=(10, 4*len(subregions)))\n",
    "text_labels = [x for x in string.ascii_lowercase[0:len(subregions)]]\n",
    "# Iterate over subregions\n",
    "for i, (o1region, o2region) in enumerate(subregions):\n",
    "    \n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    \n",
    "    # Grab all AOIs in subregion\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1region) & (aois['O2Region']==o2region)]\n",
    "    site_names_subregion = aois_subregion['RGIId'].drop_duplicates().values\n",
    "    # Grab all snowlines in subregion\n",
    "    Isubregion = [i for i in np.arange(len(snowlines)) \n",
    "                  if snowlines.loc[i, 'site_name'] in site_names_subregion]\n",
    "    snowlines_subregion = snowlines.iloc[Isubregion]\n",
    "    # Calculate AAR median and IQR trends over time\n",
    "    aar_median = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR'].median().reset_index()\n",
    "    aar_median['deciyear'] = aar_median['Year'] + (aar_median['WOY'] / 52)\n",
    "    # aar_q1 = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR'].apply(np.quantile(q=0.25))\n",
    "    # aar_q3 = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR'].apply(np.quantile(q=0.75))\n",
    "    # Grab all ERA data in subregion\n",
    "    Isubregion = [i for i in np.arange(len(eras)) \n",
    "                  if eras.loc[i, 'site_name'] in site_names_subregion]\n",
    "    eras_subregion = eras.iloc[Isubregion]\n",
    "    # Calculate PDD median and IQR trends over time\n",
    "    pdd_median = eras_subregion.groupby(by=['Year', 'WOY'])['Cumulative_Positive_Degree_Days'].median().reset_index()\n",
    "    pdd_median['deciyear'] = pdd_median['Year'] + (pdd_median['WOY'] / 52)\n",
    "    # pdd_q1 = eras_subregion.groupby(by=['Year', 'WOY'])['Cumulative_Positive_Degree_Days'].apply(np.quantile(q=0.25))\n",
    "    # pdd_q3 = eras_subregion.groupby(by=['Year', 'WOY'])['Cumulative_Positive_Degree_Days'].apply(np.quantile(q=0.75))\n",
    "\n",
    "    # Plot\n",
    "    ax[i].plot(aar_median['deciyear'], aar_median['AAR'], '.-b', linewidth=0.5, markersize=3)\n",
    "    ax[i].grid()\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_ylabel('Transient AAR', color='b')\n",
    "    ax[i].tick_params(axis='y', colors='b')\n",
    "    ax[i].set_title(text_labels[i] + ') ' + subregion_name)\n",
    "    ax2 = ax[i].twinx()\n",
    "    ax2.plot(pdd_median['deciyear'], pdd_median['Cumulative_Positive_Degree_Days'], '-m')\n",
    "    ax2.set_ylabel('$\\Sigma$PDD', color='m')\n",
    "    ax2.tick_params(axis='y', colors='m')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = 'time_series_AAR_PDDs.png'\n",
    "fig.savefig(os.path.join(figures_out_path, fig_fn), dpi=200, bbox_inches='tight')\n",
    "print('figure saved to file:', os.path.join(figures_out_path, fig_fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278abcee-2831-4e3a-a11b-53c3c37c282e",
   "metadata": {},
   "source": [
    "## Plot weather data time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94331e2-b8f3-4c91-be67-301ccec333f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "874f51b6-4503-4d61-b50e-1c1f5c9f353d",
   "metadata": {},
   "source": [
    "## Plot correlation statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5dc5e-7c6c-4259-8cb8-47ce49c3c363",
   "metadata": {},
   "source": [
    "### AAR-PDD correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522aa82-b120-4f8b-8251-0ea1d5e3db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load AAR-PDD correlations for each subregion\n",
    "corr_coeffs_fns = sorted(glob.glob(os.path.join(eras_path, 'correlation_*aar-pdd*.csv')))\n",
    "corr_coeffs = pd.DataFrame()\n",
    "for corr_coeffs_fn in corr_coeffs_fns:\n",
    "    # grab subregion name from file name\n",
    "    subregion_name = os.path.basename(corr_coeffs_fn).split('aar-pdd_')[1].split('.csv')[0]\n",
    "    # load correlation coefficients for all sites\n",
    "    corr_coeffs_subregion = pd.read_csv(corr_coeffs_fn)\n",
    "    # add subregion name to dataframe\n",
    "    corr_coeffs_subregion['Subregion'] = subregion_name    \n",
    "    # concatenate to full dataframe\n",
    "    corr_coeffs = pd.concat([corr_coeffs, corr_coeffs_subregion])\n",
    "corr_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dd756-0e05-4bc6-96d9-4eaed6072494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Plot boxplots\n",
    "palette = dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.boxplot(data=corr_coeffs, x='Subregion', y='AAR-PDD Corr. Coeff.', palette=palette, ax=ax)\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('AAR - $\\Sigma$PDD Correlation Coefficients')\n",
    "# ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# -----Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'aar-pdd_correlation_coefficients_boxplot.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file: ', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03124374-6931-413b-864f-63fdeefb61f0",
   "metadata": {},
   "source": [
    "## Plot scatter plots of AAR vs. cumulative PDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bdfa7-74d9-438e-a239-f549636f8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load all snowlines\n",
    "snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "snowlines['Date'] = snowlines['datetime'].values.astype('datetime64[D]')\n",
    "\n",
    "# -----Load all ERA data\n",
    "eras = pd.read_csv(os.path.join(eras_path, eras_fn))\n",
    "eras['Date'] = pd.to_datetime(eras['Date'], format='mixed')\n",
    "\n",
    "# -----Load all AOIs\n",
    "aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116bf17-992f-4e2a-8be1-1c30a2ac6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Fit multiple linear regression model to AAR, PDDs, and Snowfall\n",
    "# intialize full dataframe\n",
    "snowlines_eras_merged = pd.DataFrame()\n",
    "# iterate over sites\n",
    "for site_name in tqdm(snowlines['site_name'].drop_duplicates().values):\n",
    "    # subset dataframes to site\n",
    "    snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "    eras_site = eras.loc[eras['site_name']==site_name]\n",
    "    # merge snowlines and ERA dataframes\n",
    "    snowlines_eras_merged_site = snowlines_site.merge(eras_site, how='left', on='Date')\n",
    "    snowlines_eras_merged_site.rename(columns={'site_name_x': 'site_name'}, inplace=True)\n",
    "    # save in dataframe\n",
    "    snowlines_eras_merged_site = snowlines_eras_merged_site[['site_name', \n",
    "                                                             'AAR', \n",
    "                                                             'Cumulative_Positive_Degree_Days',\n",
    "                                                             'Cumulative_Precipitation_mwe',\n",
    "                                                             'Cumulative_Snowfall_mwe']]\n",
    "    snowlines_eras_merged_site.reset_index(drop=True, inplace=True)\n",
    "    snowlines_eras_merged_site.dropna()\n",
    "    # plot\n",
    "    ax.plot(snowlines_eras_merged_site['Cumulative_Positive_Degree_Days'], \n",
    "            snowlines_eras_merged_site['AAR'], \n",
    "            '.', markersize=1)\n",
    "    # concatenate to full dataframe\n",
    "    snowlines_eras_merged = pd.concat([snowlines_eras_merged, snowlines_eras_merged_site])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb28dd-7be1-493e-be1e-718e3cc13be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Fit multi-linear regression model\n",
    "snowlines_eras_merged.dropna(inplace=True)\n",
    "model = LinearRegression()\n",
    "X_columns = ['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']\n",
    "# fit model\n",
    "X = snowlines_eras_merged[X_columns]\n",
    "y = snowlines_eras_merged['AAR'].values\n",
    "model_fit = model.fit(X, y)\n",
    "# calculate R^2\n",
    "r2 = model_fit.score(X, y)\n",
    "# plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(X[X_columns[0]], X[X_columns[1]], y, '.', color='grey', markersize=0.5)\n",
    "y_pred = model_fit.predict(X)\n",
    "ax.plot(X[X_columns[0]], X[X_columns[1]], y_pred, '.k')\n",
    "ax.set_xlabel(X_columns[0])\n",
    "ax.set_ylabel(X_columns[1])\n",
    "ax.set_zlabel('AAR')\n",
    "ax.set_title('AAR = ' + str(np.round(model_fit.coef_[0], 5)) + '*$\\Sigma$PDDs ' \n",
    "             + str(np.round(model_fit.coef_[1], 5)) + '*$\\Sigma$Snowfall' \n",
    "             + '\\nR$^2$ = ' + str(np.round(r2,55)))\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'aar-pdd-snowfall_multiple_linear_regression.png')\n",
    "fig.savefig(fig_fn, dpi=200, bbox_inches='tight')\n",
    "print('figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909afb2-305a-41be-8df7-0d0eab1aca4b",
   "metadata": {},
   "source": [
    "## Plot machine learning results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fc2d0-ec6c-4997-93f1-cbc747848530",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f07615-89d4-4698-8da0-af8c3c560503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4546e0e-7dc0-4e76-8757-af26c0a5ba4b",
   "metadata": {},
   "source": [
    "## Climate/weather sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adeea9-52f3-457e-8fcf-9b4fedeb25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Define dictionary for colors of each subregion\n",
    "color_dict = {\n",
    "    'Brooks Range': 'c',\n",
    "    'Alaska Range': '#1f78b4',\n",
    "    'Aleutians': '#6d9c43',\n",
    "    'W. Chugach Mtns.': '#264708',\n",
    "    'St. Elias Mtns.': '#fb9a99',\n",
    "    'N. Coast Ranges': '#e31a1c',\n",
    "    'N. Rockies': '#cab2d6',\n",
    "    'N. Cascades': '#fdbf6f',\n",
    "    'C. Rockies': '#9657d9',\n",
    "    'S. Cascades': '#ff7f00',\n",
    "    'S. Rockies': '#6a3d9a'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04e5a2-121b-4eeb-a617-4f529336ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_path = os.path.join(scm_path, 'machine_learning')\n",
    "sens_run_fns = sorted(glob.glob(os.path.join(ml_path, 'aar_sensitivity_tests*.csv')))\n",
    "\n",
    "# -----Define regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# -----Set up figure\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 12), gridspec_kw={'width_ratios': [3, 1]})\n",
    "ax = ax.flatten()\n",
    "\n",
    "# -----Iterate over PDDs and Snowfall\n",
    "for i, column in enumerate(['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']):\n",
    "    # Grab file names for column\n",
    "    sens_run_column_fns = [x for x in sens_run_fns if column in x]\n",
    "    # Iterate over file names\n",
    "    for j, fn in enumerate(sens_run_column_fns):\n",
    "        # grab subregion name from file name\n",
    "        subregion_name = fn.split('aar_sensitivity_tests_')[1].split('_' + column)[0]\n",
    "        color = color_dict[subregion_name]\n",
    "        # load file\n",
    "        sens_run = pd.read_csv(fn)\n",
    "        # remove mean conditions row\n",
    "        sens_run = sens_run.loc[sens_run['scenario']!='mean']\n",
    "        # fit linear regression to cumsum(PDDs) and AAR\n",
    "        model_fit = model.fit(sens_run[column].values.reshape(-1, 1), \n",
    "                              sens_run['AAR'].values.reshape(-1, 1))\n",
    "        # grab linear fit coefficient\n",
    "        coef = model_fit.coef_[0][0]\n",
    "        # add to DataFrame\n",
    "        fit_df = pd.DataFrame({'Subregion': [subregion_name],\n",
    "                               'Color': color,\n",
    "                               'Linear_Fit_Coefficient': [coef]})\n",
    "        fits_df = pd.concat([fits_df, fit_df])\n",
    "        # plot\n",
    "        lines = ax[i*2].plot(sens_run[column], \n",
    "                             sens_run['AAR'], '-', label=subregion_name, color=color)\n",
    "        ax[(i*2)+1].plot(j, coef, 'o', color=color, markersize=5, label=subregion_name)\n",
    "    ax[i*2].set_ylabel('AAR')\n",
    "    ax[i*2].set_xlabel(column)\n",
    "    ax[i*2].grid()\n",
    "    ax[(i*2)+1].set_ylabel('Linear fit coefficient')\n",
    "    ax[(i*2)+1].grid()\n",
    "\n",
    "leg = ax[1].legend(loc='right', bbox_to_anchor=[1.9, -0.2, 0.2, 0.2])\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71460f2-3914-49a9-9635-851437e13fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a673434-4a15-475f-9045-2f9f205fb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingRegressor()\n",
    "model_fit = model.fit(sens_run[column].values.reshape(-1, 1), sens_run['AAR'].values.reshape(-1, 1))\n",
    "model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c627e42-c29d-4df3-a8ce-50a9243db773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
