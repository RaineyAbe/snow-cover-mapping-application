{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fee1d3-89a0-4332-91af-77fbee8cc9ed",
   "metadata": {},
   "source": [
    "# Calculate median weekly trends in snow cover for each study site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ded5b9-6a41-4e78-aa7d-dbec024f6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from scipy.stats import iqr\n",
    "from shapely import wkt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4325a8-0c88-45c3-b6a7-5fd875a8205f",
   "metadata": {},
   "source": [
    "## Define paths in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89c723-245e-4769-8633-a503980f94e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "figures_out_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d398a-23a0-4431-aee8-5fe622f39b68",
   "metadata": {},
   "source": [
    "## Load compiled glacier boundaries (AOIs) and climate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97356-a7e0-4c83-af84-ceee07b044fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load AOIs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All glacier boundaries loaded from file.')\n",
    "\n",
    "# -----Load climate clusters\n",
    "clusters_fn = os.path.join(scm_path, 'analysis', 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Clusters loaded from file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db1685",
   "metadata": {},
   "source": [
    "## Compile snow cover stats for individual sites if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge snow cover stats files for each site if necessary\n",
    "\n",
    "# # Define columns to save\n",
    "# out_cols = ['RGIId', 'datetime', 'source', 'SCA_m2', 'AAR', 'ELA_from_AAR_m', 'snowline_elevs_m', 'snowline_elevs_median_m', 'snowline_geometry']\n",
    "\n",
    "# # Define function to convert lists of X and Y coordinates to a LineString and transform to WGS84\n",
    "# def create_linestring_wgs84(x_coords, y_coords, transformer):\n",
    "#     # Create list of (x, y) coordinate tuples\n",
    "#     points = list(zip(x_coords, y_coords))\n",
    "#     # Create LineString in UTM\n",
    "#     line = LineString(points)\n",
    "#     # Transform to WGS84\n",
    "#     line_wgs84 = LineString([transformer.transform(x, y) for x, y in line.coords])\n",
    "#     return line_wgs84\n",
    "\n",
    "# # Iterate over sites\n",
    "# for rgi_id in tqdm(rgi_ids):\n",
    "#     # Define output file name\n",
    "#     scs_fn = os.path.join(study_sites_path, rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "#     if not os.path.exists(scs_fn):\n",
    "#         # Initialize dataframe\n",
    "#         scs = pd.DataFrame()\n",
    "#         # Get snow cover stats file names\n",
    "#         sc_fns = sorted(glob.glob(os.path.join(study_sites_path, rgi_id, 'imagery', 'snowlines', '*_snowline.csv')))\n",
    "#         # Merge files\n",
    "#         for fn in sc_fns:\n",
    "#             if 'PlanetScope' not in fn:\n",
    "#                 sc = pd.read_csv(fn)\n",
    "#                 scs = pd.concat([scs, sc])\n",
    "#         # Merge redundant columns\n",
    "#         cols = list(scs.keys())\n",
    "#         if ('site_name' in cols) & ('RGIId' in cols):\n",
    "#             scs['RGIId'] = scs['RGIId'].fillna(scs['site_name'])\n",
    "#         elif 'site_name' in cols:\n",
    "#             scs.rename(columns={'site_name': 'RGIId'}, inplace=True)\n",
    "#         if ('dataset' in cols) & ('source' in cols):\n",
    "#             scs['source'] = scs['source'].fillna(scs['dataset'])\n",
    "#         elif 'dataset' in cols:\n",
    "#             scs.rename(columns={'dataset': 'source'}, inplace=True)\n",
    "#         scs.reset_index(drop=True, inplace=True)\n",
    "#         # Ensure coordinate lists are correctly formatted\n",
    "#         scs['snowlines_coords_X'] = scs['snowlines_coords_X'].apply(lambda x: literal_eval(x) if x != \"[]\" else [])\n",
    "#         scs['snowlines_coords_Y'] = scs['snowlines_coords_Y'].apply(lambda x: literal_eval(x) if x != \"[]\" else [])\n",
    "#         # Get the UTM CRS for transformation\n",
    "#         crs_utm = scs['HorizontalCRS'].drop_duplicates().dropna().values[0]\n",
    "#         transformer = Transformer.from_crs(crs_utm, \"EPSG:4326\", always_xy=True)\n",
    "#         # Create and transform snowline geometries\n",
    "#         scs['snowline_geometry'] = scs.apply(lambda row: create_linestring_wgs84(row['snowlines_coords_X'], row['snowlines_coords_Y'], transformer), axis=1)\n",
    "#         # Select the relevant columns\n",
    "#         scs = scs[out_cols]\n",
    "#         # Save merged, adjusted dataframe\n",
    "#         scs.to_csv(scs_fn, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada9ced-080b-493c-bdaf-8ac48a51bfb1",
   "metadata": {},
   "source": [
    "## Calculate weekly median trends for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23ff0d-5260-47ad-86d0-9a053e716cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scs_medians_fn = os.path.join(scm_path, 'analysis', 'weekly_median_snow_cover_stats.csv') \n",
    "if not os.path.exists(scs_medians_fn):\n",
    "    # determine columns to calculate weekly stats\n",
    "    columns = ['AAR', 'snowline_elevs_median_m', 'SCA_m2', 'ELA_from_AAR_m']\n",
    "    scs_medians = pd.DataFrame()\n",
    "\n",
    "    # Iterate over study sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Compile all snow cover stats\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "        scs_site = pd.read_csv(scs_fn)\n",
    "\n",
    "        # Add WOY column\n",
    "        if 'datetime' not in scs_site.keys():\n",
    "            print(f'Error with {rgi_id}')\n",
    "            continue\n",
    "        scs_site['datetime'] = pd.to_datetime(scs_site['datetime'], format='mixed')\n",
    "        scs_site['WOY'] = scs_site['datetime'].dt.isocalendar().week\n",
    "        \n",
    "        # calculate weekly quartile trends\n",
    "        q1 = scs_site[['WOY'] + columns].groupby(by='WOY').quantile(0.25)\n",
    "        q1.columns = [x + '_P25' for x in q1.columns]\n",
    "        q2 = scs_site[['WOY'] + columns].groupby(by='WOY').quantile(0.5)\n",
    "        q2.columns = [x + '_P50' for x in q2.columns]\n",
    "        q3 = scs_site[['WOY'] + columns].groupby(by='WOY').quantile(0.75)\n",
    "        q3.columns = [x + '_P75' for x in q3.columns]\n",
    "        qs = pd.merge(q1, pd.merge(q2, q3, on='WOY'), on='WOY')\n",
    "        qs = qs.reindex(sorted(qs.columns), axis=1)\n",
    "        qs['WOY'] = qs.index\n",
    "        qs['RGIId'] = rgi_id\n",
    "        # concatenate to medians dataframe\n",
    "        scs_medians = pd.concat([scs_medians, qs])\n",
    "    # save to file\n",
    "    scs_medians.to_csv(scs_medians_fn, index=False)\n",
    "    print('Median weekly snow trends saved to file: ', scs_medians_fn)\n",
    "        \n",
    "else:\n",
    "    scs_medians = pd.read_csv(scs_medians_fn)\n",
    "    print('Median weekly snow cover trends loaded from file.')\n",
    "    \n",
    "scs_medians\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec25ef-248b-4fff-8a5e-b7ac70f2827e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Compile minimum snow cover median statistics\n",
    "min_snow_cover_stats_fn = os.path.join(scm_path, 'analysis', 'min_snow_cover_stats.csv') \n",
    "# check if exists in directory\n",
    "if not os.path.exists(min_snow_cover_stats_fn):\n",
    "    # initialize dataframe for RGI stats and minimum snow cover statts\n",
    "    min_snow_cover_stats = pd.DataFrame()\n",
    "    \n",
    "    # iterate over site names in median snow cover stats dataframe\n",
    "    for rgi_id in tqdm(sorted(scs_medians['RGIId'].drop_duplicates().values)):\n",
    "        # grab weekly median snowline stats for site\n",
    "        scs_medians_site = scs_medians.loc[scs_medians['RGIId']==rgi_id]\n",
    "        # calculate min median stats\n",
    "        median_columns = [x for x in scs_medians.columns if 'P50' in x]\n",
    "        df = pd.DataFrame()\n",
    "        for column in median_columns:\n",
    "            # Take the max ELA and snowline elevations\n",
    "            if (column=='ELA_from_AAR_m_P50') or (column=='snowline_elevs_median_m_P50'):\n",
    "                Imax = scs_medians_site[column].argmax()\n",
    "                max_value = scs_medians_site.iloc[Imax][column]\n",
    "                max_WOY = scs_medians_site.iloc[Imax]['WOY']\n",
    "                df[column+'_max'] = [max_value]\n",
    "                df[column+'_max_WOY'] = [max_WOY]\n",
    "            # Take the minimum AAR and SCA\n",
    "            else:\n",
    "                Imin = scs_medians_site[column].argmin()\n",
    "                min_value = scs_medians_site.iloc[Imin][column]\n",
    "                min_WOY = scs_medians_site.iloc[Imin]['WOY']\n",
    "                df[column+'_min'] = [min_value]\n",
    "                df[column+'_min_WOY'] = [min_WOY]\n",
    "            df['RGIId'] = rgi_id\n",
    "        # save the ~September 1 observation for reference\n",
    "        if 39 in scs_medians_site['WOY'].values:\n",
    "            df[[column+'_WOY39' for column in columns]] = scs_medians_site.loc[scs_medians_site['WOY']==39, columns].values\n",
    "        else:\n",
    "            # If WOY 39 not in df, interpolate linearly from WOY 38 and 40\n",
    "            df[[column+'_WOY39' for column in columns]] = (scs_medians_site.loc[scs_medians_site['WOY']==38, columns].values\n",
    "                                                           + scs_medians_site.loc[scs_medians_site['WOY']==40, columns].values)/2\n",
    "        # concatenate to full dataframe\n",
    "        min_snow_cover_stats = pd.concat([min_snow_cover_stats, df], axis=0)\n",
    "\n",
    "    # save to file\n",
    "    min_snow_cover_stats.to_csv(min_snow_cover_stats_fn, index=False)\n",
    "    print('Minimum median snow cover stats saved to file: ', min_snow_cover_stats_fn)\n",
    "        \n",
    "else:\n",
    "    # load from file\n",
    "    min_snow_cover_stats = pd.read_csv(min_snow_cover_stats_fn)\n",
    "    print('Minimum median snow cover stats loaded from file.')\n",
    "\n",
    "# reformat as GeoDataFrame\n",
    "min_snow_cover_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ce47e-be94-4518-bf30-c319db2e17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subregion and cluster columns\n",
    "min_snow_cover_stats[['Subregion', 'clustName']] = '', ''\n",
    "for rgi_id in min_snow_cover_stats['RGIId'].drop_duplicates().values:\n",
    "    subregion = aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values[0]\n",
    "    clustName = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==rgi_id, ['Subregion', 'clustName']] = subregion, clustName\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sns.kdeplot(data=min_snow_cover_stats, x='AAR_P50_min_WOY', palette='mako', cumulative=True, hue='Subregion', \n",
    "             ax=ax[0])\n",
    "sns.kdeplot(data=min_snow_cover_stats, x='AAR_P50_min_WOY', cumulative=True, hue='clustName', \n",
    "             ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781acfe-3e6d-46a1-8962-1086332f34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "min_snow_cover_stats.groupby(by=['Subregion', 'clustName'])['AAR_P50_min'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703d812-c2ad-482a-8518-f5ef4fda44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_snow_cover_stats.groupby(by=['clustName'])['ELA_from_AAR_m_P50_max_WOY'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2293b96-3a07-4559-9728-4569893eef51",
   "metadata": {},
   "source": [
    "## Assess interannual variability in AAR magnitude and timing at each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ac151-e7a3-419a-b51f-2c19fcc9a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_var_stats_fn = os.path.join(os.path.join(scm_path, 'analysis', 'minimum_snow_cover_stats_interannual_variability_2016-2023.csv'))\n",
    "if os.path.exists(aar_var_stats_fn):\n",
    "    aar_var_stats = pd.read_csv(aar_var_stats_fn)\n",
    "    print('AAR interannual variability stats loaded from file.')\n",
    "\n",
    "else:\n",
    "    aar_var_stats = pd.DataFrame()\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load snow cover stats\n",
    "        scs_fns = sorted(glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, \n",
    "                                                'imagery', 'snowlines', '*.csv')))\n",
    "        scs_site = pd.DataFrame()\n",
    "        for fn in scs_fns:\n",
    "            sc = pd.read_csv(fn)\n",
    "            scs_site = pd.concat([scs_site, sc])\n",
    "        scs_site.reset_index(drop=True, inplace=True)\n",
    "        # Add Year and WOY columns\n",
    "        if 'datetime' not in scs_site.keys():\n",
    "            print(f'Error with {rgi_id}')\n",
    "            continue\n",
    "        scs_site['datetime'] = pd.to_datetime(scs_site['datetime'], format='mixed')\n",
    "        scs_site['Year'] = scs_site['datetime'].dt.isocalendar().year\n",
    "        scs_site['WOY'] = scs_site['datetime'].dt.isocalendar().week\n",
    "        \n",
    "        # subset to 2016\n",
    "        scs_site = scs_site.loc[scs_site['Year'] >= 2016]\n",
    "        # identify annual AAR magnitudes and WOY timing\n",
    "        annual_mins_site = scs_site.groupby('Year')['AAR'].idxmin().reset_index()\n",
    "        annual_mins_site.rename(columns={'AAR': 'Imin'}, inplace=True)\n",
    "        annual_mins_site['AAR'] = [scs_site.loc[i, 'AAR'] for i in annual_mins_site['Imin'].values]\n",
    "        annual_mins_site['WOY'] = [scs_site.loc[i, 'WOY'] for i in annual_mins_site['Imin'].values]\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                           'AAR_min': [annual_mins_site['AAR'].min()],\n",
    "                           'AAR_max': [annual_mins_site['AAR'].max()],\n",
    "                           'AAR_median': [annual_mins_site['AAR'].median()],\n",
    "                           'AAR_IQR': [iqr(annual_mins_site['AAR'])],\n",
    "                           'WOY_min': [annual_mins_site['WOY'].min()],\n",
    "                           'WOY_max': [annual_mins_site['WOY'].max()],\n",
    "                           'WOY_median': [annual_mins_site['WOY'].median()],\n",
    "                           'WOY_IQR': [iqr(annual_mins_site['WOY'])]})  \n",
    "        aar_var_stats = pd.concat([aar_var_stats, df])\n",
    "    \n",
    "    aar_var_stats.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save to file\n",
    "    aar_var_stats.to_csv(aar_var_stats_fn, index=False)\n",
    "    print('AAR interannual variability stats saved to file:', aar_var_stats_fn)\n",
    "\n",
    "aar_var_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a3a37-6f68-4f8d-8a7c-611ed1511b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Print stats\n",
    "aar_var_stats['AAR_range'] = aar_var_stats['AAR_max'] - aar_var_stats['AAR_min']\n",
    "print(f\"AAR range for all sites: {aar_var_stats['AAR_range'].median()} +/- {iqr(aar_var_stats['AAR_range'])}\\n\")\n",
    "# print('By subregion:')\n",
    "# print('Median')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['AAR_range'].median())\n",
    "# print('\\n')\n",
    "# print('IQR')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['AAR_range'].apply(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1127fa-5146-4fad-98fd-344c66a8e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_var_stats['WOY_range'] = aar_var_stats['WOY_max'] - aar_var_stats['WOY_min']\n",
    "print(f\"AAR TIMING range for all sites: {aar_var_stats['WOY_range'].median()} +/- {iqr(aar_var_stats['WOY_range'])}\\n\")\n",
    "# print('By subregion:')\n",
    "# print('Median')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['WOY_range'].median())\n",
    "# print('\\n')\n",
    "# print('IQR')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['WOY_range'].apply(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1886-3983-4022-afa2-5b8e32d52af7",
   "metadata": {},
   "source": [
    "## Identify the approximate start and end of the melt season in each subregion from ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1684a2-53a8-44e4-8d9f-c3f53fdb483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_season_fn = os.path.join(scm_path, 'analysis', 'melt_season_timing.csv')\n",
    "\n",
    "if not os.path.exists(melt_season_fn):\n",
    "    melt_season_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load ERA data\n",
    "        era_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'ERA', f'{rgi_id}_ERA5_daily_means.csv')\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era['Date'] = pd.to_datetime(era['Date'])\n",
    "    \n",
    "        # Add WOY column\n",
    "        era['WOY'] = era['Date'].dt.isocalendar().week\n",
    "    \n",
    "        # Calculate weekly medians for 2013â€“2022\n",
    "        era = era.loc[era['year'] > 2012]\n",
    "        if '.geo' in era.keys():\n",
    "            era = era.drop(columns=['.geo'])\n",
    "        era_weekly_median = era.groupby('WOY').median().reset_index()\n",
    "    \n",
    "        # Estimate start and end of melt seasons\n",
    "        # Start = positive PDDs\n",
    "        try:\n",
    "            woy_start = era_weekly_median.loc[era_weekly_median['positive_degree_days_annual_sum'] > 0, 'WOY'].values[0]\n",
    "        except:\n",
    "            woy_start = 52\n",
    "        # End = after July, 0 PDDs, positive snowfall\n",
    "        woy_end = era_weekly_median.loc[(era_weekly_median['WOY'] > 30) \n",
    "                                     & (era_weekly_median['positive_degree_days'] == 0) \n",
    "                                     & (era_weekly_median['mean_snowfall_sum'] > 0), 'WOY'].values[0]\n",
    "        \n",
    "        # Add to full dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id], \n",
    "                           'melt_season_start_WOY': [woy_start],\n",
    "                           'melt_season_end_WOY': [woy_end],\n",
    "                          })\n",
    "        melt_season_df = pd.concat([melt_season_df, df], axis=0)\n",
    "    \n",
    "    # Save to file\n",
    "    melt_season_df.reset_index(drop=True, inplace=True)\n",
    "    melt_season_df.to_csv(melt_season_fn, index=False)\n",
    "    print('Melt season timing CSV saved to file:', melt_season_fn)\n",
    "\n",
    "else:\n",
    "    melt_season_df = pd.read_csv(melt_season_fn)\n",
    "    print('Melt season timing CSV loaded from file.')\n",
    "\n",
    "melt_season_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8218cb7-b350-4169-9cbb-db194c816c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results\n",
    "\n",
    "# Add subregion and cluster columns\n",
    "if 'Subregion' not in melt_season_df.keys():\n",
    "    melt_season_df['Subregion'] = ''\n",
    "    melt_season_df['clustName'] = ''\n",
    "    for rgi_id in melt_season_df['RGIId'].drop_duplicates().values:\n",
    "        melt_season_df.loc[melt_season_df['RGIId']==rgi_id, 'Subregion'] = aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values\n",
    "        melt_season_df.loc[melt_season_df['RGIId']==rgi_id, 'clustName'] = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values\n",
    "\n",
    "nsubregions = len(melt_season_df['Subregion'].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(nsubregions, 1, figsize=(8, nsubregions*4))\n",
    "for i, subregion in enumerate(melt_season_df['Subregion'].drop_duplicates().values):\n",
    "    melt_season_subregion_df = melt_season_df.loc[melt_season_df['Subregion']==subregion]\n",
    "    ax[i].hist(melt_season_subregion_df['melt_season_start_WOY'], bins=20, facecolor='m', alpha=0.5)\n",
    "    ax[i].axvline(melt_season_subregion_df['melt_season_start_WOY'].mean(), color='m', linewidth=2)\n",
    "    ax[i].hist(melt_season_subregion_df['melt_season_end_WOY'], bins=20, facecolor='b', alpha=0.5)\n",
    "    ax[i].axvline(melt_season_subregion_df['melt_season_end_WOY'].mean(), color='b', linewidth=2)\n",
    "    ax[i].set_title(subregion)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1412c6-2c07-4d4b-a83d-04934f161dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-snow-cover-mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
